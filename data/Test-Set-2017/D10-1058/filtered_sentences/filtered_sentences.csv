id,sentence
7,"We use gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in ibm model 4."
8,"Ibm models and the hidden markov model (hmm) for word alignment are the most influential statistical word alignment models (brown et al., 1993; vogel et al., 1996; och and ney, 2003)."
10,"Ibm model 1 uses only lexical information; ibm model 2 and the hidden markov model take advantage of both lexical and locality information; ibm models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago."
11,Recent experiments on large datasets have shown that the performance of the hidden markov model is very close to ibm model 4.
12,"Nevertheless, we believe that ibm model 4 is essentially a better model because it exploits the fertility of words in the tar get language."
26,"There have been works that try to simulate fertility using the hidden markov model (toutanova et al., 2002; deng and byrne, 2005), but we prefer to model fertility directly."
27,Our model is a coherent generative model that combines the hmm and ibm model 4.
32,"Our distortion parameters are similar to ibm model 2 and the hmm, while ibm model 4 uses inverse distortion (brown et al., 1993)."
38,"(1993) and och and ney (2003) first compute the viterbi alignments for simpler models, then consider only some neighbors of the viterbi alignments for modeling fertility."
40,Moore (2004) also suggested adding multiple empty words to the target sentence for ibm model 1.
46,"We use the markov chain monte carlo (mcmc) method for training and decoding, φi = j=1 δ(aj , i) which has nice probabilistic guarantees."
48,(2008) applied the markov chain monte carlo method to word alignment for machine translation; they do not model word fertility.
71,"−1 aligns with one ofp (f j |e2i +1) = ""£ j p (aj , f j |e2i +1)."
75,"The absolute position in the hmm is not important, because we re-parametrize the distortion probability in terms of the distance between adjacent alignment points (vogel et al., 1996; och and ney, 2003): = p (j |e2i +1) n p (aj , fj |f j−1, aj−1, e2i +1) c(i − i′) 1 j=1 1 1 1 p (i|i′, i ) = ""£ i′′ c(i′′ − i′) j = p (j |e2i +1) n p (aj |f j−1, aj−1, e2i +1) × where c( ) is the count of jumps of a given distance."
81,"The hmm is p (aj , f j |e2i +1) = p (j |i ) n p (f |e ) less likely to have this garbage collector problem be 1 1 1 (2i + 1)j j=1 j aj cause of the alignment probability constraint."
82,"However, fertility is an inherent cross language propertythe hidden markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: p (aj , f j |e2i +1) = 1 1 1 j p (j |i ) n p (aj |aj−1, i )p (fj |ea ) j=1 in order to make the hmm work correctly, we enforce the following constraints (och and ney, 2003): and these two models cannot assign consistent fertility to words."
84,"Because the hmm performs much better than ibm model 1, we expect that the fertility hidden markov model will perform much better than the fertility ibm model 1."
88,"But we want to point out that the locality property modeled in the hmm is missing in ibm model 3, and is modeled invertedly in ibm model 4."
90,"Our fertility ibm model 1 and fertility hmm are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: p (φi , φǫ, aj , f j |e2i +1); 1 1 1 1 are further away from the mean have low probability."
93,"In the fertility ibm model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: p (φi , φǫ, aj , f j |e2i +1) the data likelihood can be computed by 1 1 1 i φi 1 λ(ei ) summing over fertilities and alignments: n λ(ei) e− × p (f j |e2i +1) = ""£ i j p (φi , φǫ, aj , f j |e2i +1)."
98,"Now p (φi , φǫ, aj , f j |e2i +1) can be decomposed 1 (2i + 1)j n p (fj | j=1 eaj ) (1) 1 1 1 1 in the following way: p (φi , φǫ, aj , f j |e2i +1) in the fertility hmm, we assume that the distor tion probability depends only on the previous alignment and the length of the target sentence, and that 1 1 1 1 = p (φi |e2i +1)p (φǫ|φi , e2i +1) × 1 1 1 1 j the lexical probability depends only on the aligned target word: n p (aj , fj |f j−1, aj−1, e2i +1, φi , φǫ) j=1 1 1 1 1 p (φi , φǫ, aj , f j |e2i +1) = n λ(ei) e−λ(ei ) 1 1 1 i φ 1 λ(e ) φi! × = n λ(ei) i e− i i=1 (i λ(ǫ))φǫ e−i λ(ǫ) φǫ!"
103,"Any fertility value p (f j |e2i +1) = p (aj , f j |e2i +1) has a nonzero probability, but fertility values that 1 1 1 1 1 j 1 where p (aj , f j |e2i +1) auxiliar y functio n is: l(p (f |e), p (a|a ), λ(e), ξ1(e) , ξ2(a )) 1 1 1 = p (φi , φǫ, aj , f j |e2i +1) = p˜ ′ aj e 2i +1, f j ) log ′ p (aj , f j | e2i +1) 1 1 ,φǫ 1 1 1 1 1 1 j 1 1 1 1 ≈ p (φi , φǫ, aj , f j |e2i +1) × − ξ1(e)( p (f |e) − 1) 1 1 1 1 i  j  e f n δ  i=1 j=1 δ(aj , i), φi × − ξ2(a′)( a′ a p (a|a′) − 1)  2i +1 j  because p (aj , f j |e2i +1) is in the exponential 1 1 1 δ  i=i +1 j=1 δ(aj , i), φǫ (3) family, we get a closed form for the parameters from expected counts: in the last two lines of equation 3, φǫ and each p (f |e) = ""£s c (f |e; f (s), e(s)) (4) φi are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments."
109,We estimate the parameters by maximizing p (f j |e2i +1) using the expectation maximization these equations are for the fertility hidden markov model.
112,"The the em algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential."
119,"Instead, we do “batch learning”: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (e-step); then combine all the counts together and update the parameters (m- step)."
120,This is analogous to what ibm models and end end we also consider initializing the alignments using the hmm viterbi algorithm in the e-step.
126,"For the fertility hidden markov model, updating p (aj , f j |e2i +1) whenever we change the alignment 1 1 1 aj can be done in constant time, so the complexity of choosing t samples for all aj (j = 1, 2,"
129,"Surprisingly, we can achieve better results than the hmm by computing as few as 1 sample for each alignment, so the fertility hidden markov model is much faster than the hmm."
135,"Interestingly, we found that throwing away the fertility and using the hmm viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster."
181,"In fact, with just 1 sample for each alignment, our model archives lower aer than the hmm, and runs more than 5 times faster than the hmm."
182,It is possible to use sampling instead of dynamic programming in the hmm to reduce the training time with no decrease in aer (often an increase).
183,"We conclude that the fertility hmm not only has better aer results, but also runs faster than the hidden markov model."
192,The markov chain monte carlo method used in our model is more principled than the heuristic-based neighborhood method in ibm model 4.
