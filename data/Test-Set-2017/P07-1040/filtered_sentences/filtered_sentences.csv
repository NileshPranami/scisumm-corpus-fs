id,sentence
2,"Recently, confusion network decoding has been applied in machine translation system combination."
3,"Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs."
4,This paper describes an improved confusion network based method to combine outputs from multiple mt systems.
5,"In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring."
7,"A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including ter, bleu and meteor."
9,System combination has been shown to improve classification performance in various tasks.
15,"In speech recognition, confusion network decoding (mangu et al., 2000) has become widely used in system combination."
16,"Unlike speech recognition, current statistical machine translation (mt) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems."
17,"The idea of combining outputs from different mt systems to produce consensus translations in the hope of generating better translations has been around for a while (frederking and nirenburg, 1994)."
18,"Recently, confusion network decoding for mt system combination has been proposed (bangalore et al., 2001)."
20,"In (bangalore et al., 2001), levenshtein alignment was used to generate the network."
21,"As opposed to speech recognition, the word order between two correct mt outputs may be different and the levenshtein alignment may not be able to align shifted words in the hypotheses."
22,"In (matusov et al., 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using giza++ (och and ney, 2003)."
26,"Qc 2007 association for computational linguistics potheses in (sim et al., 2007)."
28,"Also, a more heuristic alignment method has been proposed in a different system combination approach (jayaraman and lavie, 2005)."
35,"This work was extended in (rosti et al., 2007) by introducing system weights for word confidences."
36,"However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton."
37,"In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average ter scores between the skeleton and other hypotheses."
41,"In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences."
45,"Tuning is fully automatic, as opposed to (matusov et al., 2006) where global system weights were set manually.this paper is organized as follows."
47,Section 3 describes confusion network decoding for mt system combination.
54,"Similarly, full test set scores are obtained by accumulating counts over all hypothesis and reference pairs."
58,"It has been argued that meteor correlates better with human judgment due to higher weight on recall than precision (banerjee and lavie, 2005)."
60,"The second term is a fragmentation penalty which penalizes the harmonic mean by a factor of up to when ; i.e., there are no matching -grams higher than"
71,"Based on vote counts, there are three alternatives in the example: “cat sat on the mat”, “cat on the mat” and “cat sitting on the mat”, each having accumulated 10 votes."
73,Ric since it is based on the rate of edits required to transform the hypothesis into the reference.
83,Confusion network decoding in mt has to pick one hypothesis as the skeleton which determines the word order of the combination.
96,"It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (sim et al., 2007)."
97,"When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis."
98,"In (rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis."
99,"Due to the computational burden of the ter alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned."
103,"In (rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including null words, given the th source sentence was given by (5) word-level decoding."
109,"Features to address the issue with ungrammatical hypotheses and allow language model expansion and re-scoring, the hypothesis confidence computation is modified."
111,The word confidences were increased by if the word aligns between nodes and in the network.
113,The last term controls the number of null words generated in the output and may be viewed as an insertion penalty.
118,"First, the decoding can generate ungrammatical hypotheses due to alignment errors and phrases broken by the where is the language model weight, is the lm log-probability and is the number of words in the hypothesis"
127,"After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized."
136,The prior probabilities in the arcs leaving the first node will be multiplied by the corresponding system weights which guarantees that a path through a network generated around a -best from a system with a zero weight will not be chosen.
141,"However, the optimization experiments showed that the best performance was obtained by having a smoothing factor of 1 which is equivalent to the original priors."
147,"However, this approach did not include sentence 1 na."
150,"The optimization of the system and feature weights may be carried out using -best lists as in (ostendorf et al., 1991)."
151,"A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features."
152,The -best list may be reordered using the sentence-level posteriors from equation 6 for the th source sentence and the corresponding th hypothesis
155,"In this work, modified powell’s method as proposed by (brent, 1973) is used."
160,"Since the -best list represents only a small portion of all hypotheses in the confusion network, the optimized weights from one iteration may be used to generate a new -best list from the lattice for the next iteration."
162,"The same powell’s method has been used to estimate feature weights of a standard feature-based phrasal mt decoder in (och, 2003)."
164,"In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used."
165,"The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (rosti et al., 2007) on the arabic to english and chinese to english nist mt05 tasks."
166,"Six mt systems were combined: three (a,c,e) were phrase- based similar to (koehn, 2004), two (b,d) were hierarchical similar to (chiang, 2005) and one (f) was syntax-based similar to (galley et al., 2006)."
204,An improved confusion network decoding method combining the word posteriors with arbitrary features was presented.
209,"Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights."
211,"Compared to the baseline from (rosti et al., 2007), the new method improves the bleu scores significantly."
217,The improved confusion network decoding approach allows arbitrary features to be used in the combination.
