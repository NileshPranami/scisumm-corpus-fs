%
% File acl-hlt2011.tex
%
% Contact: gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt]{article}
\usepackage{tac2014}
\usepackage{times}		
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[hyphens]{url}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{10.5cm}    % Expanding the titlebox

\title{The Computational Linguistics Summarization Pilot Task}

% Names appear in alphabetical order of last names except Kokil Jaidka
% who we all agree to be the first author of this work
% Participating teams may prefer a different order among the authors 
% from their team. Please let us know in this case.


% Min: please reformat to take up less space.  See for example:
% http://www.comp.nus.edu.sg/~kanmy/papers/airs2014.pdf
% in wing.nus/GITRepositories/zhanghc/AIRS2014.git for LaTeX source formatting
% Min: use \thanks for add in the other authoring information

\author{Kokil Jaidka$^{1}\thanks{\hspace{.2cm}Authors appear in alphabetical order, with the exception of the coordinator of the task, whom is given the first authorship.} $ , Muthu Kumar Chandrasekaran$^{2}$, Rahul Jha$^{3}$, Christopher Jones$^{4}$ \\ {\bf Min-Yen Kan}$^{2,5}${\bf , Ankur Khanna}$^{2}${\bf , Diego Moll\'{a}-Aliod}$^{4}${\bf , Dragomir R. Radev}$^{3}$, \\ {\bf Francesco Ronzano}$^{6}$ and {\bf Horacio Saggion}$^{6}$ \\ 
\\
$^1$ Wee Kim Wee School of Communication \& Information, Nanyang Technological University, Singapore \\
$^2$ Web, IR \/ NLP Group, School of Computing, National University of Singapore, Singapore \\
$^3$ School of Information, University of Michigan, USA\\
$^4$ Division of Information \& Communication Sciences, Computing Department, Macquarie University, Australia \\
$^5$ Interactive and Digital Media Institute, National University of Singapore, Singapore \\
$^6$ Universitat Pompeu Fabra, Barcelona, Spain }

%rahul jha clair_umich
%dragomir radev clair_umich
%kokil jaidka ntu
%muthu nus
%ankur nus
%Francesco Ronzano, TALN Group, Universitat Pompeu Fabra, Barcelona - EMAIL: francesco.ronzano@upf.edu
%Horacio Saggion, TALN Group, Universitat Pompeu Fabra, Barcelona - EMAIL: horacio.saggion@upf.edu
%christopher jones and diego, mcq univ

\date{}

\begin{document}
\maketitle
\begin{abstract}
The Computational Linguistics (CL) Summarization Pilot Task was a
pilot shared task to use citations to create summaries of scholarly
research publications in the domain of computational linguistics.  We
describe the background for the task, corpus construction, evaluation
methods for the pilot and survey the participating systems and their
preliminary results.  The experience gleaned from the pilot will
assist in the proper organization of future shared task where
difficulties with annotations and scale can be addressed. The annotated 
development corpus used for this pilot task is available for download 
here: \begin{sloppypar}
\url{https://github.com/WING-NUS/scisumm-corpus}
\end{sloppypar}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

% Min: Ask the other groups if they have any grants that need to be
% acknowledged, which should appear in the acknowledgements here on
% the title page.
This paper describes the evolution and design of the Computational
Linguistics (CL) pilot task for the summarization of computational 
linguistics research papers sampled from the Association of 
Computational Linguistics' (ACL) anthology. This task was run 
concurrently with the Text Analysis Conference 2014 (TAC '14), 
although not formally affiliated with it. This shared task shares 
the same basic structure and guidelines with the formal TAC 2014 
Biomedical Summarization (BiomedSumm) track. A training corpus 
``topics'' from CL research papers was released, each comprising 
a reference paper along with some sampled papers that cited the 
reference paper. Participants were invited to enter their systems
in a task-based evaluation, similar to BiomedSumm.

This paper will describe the participating systems and survey their 
results from the task-based evaluation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
Recent work \cite{mohammad2009,abu2011} in scientific document
summarization have used citation sentences (also known as {\it
  citances}) from citing papers to create a multi document summary of
the reference paper (RP).
%Muthu: looks like we need to properly integrate this into an argument 
%/ motivation

As proposed by \cite{vu2010,hoang2010} the summarization can be
decomposed into finding the relevant documents; in this case, the
citing papers (CPs), then selecting sentences from those papers that
cite and justify the citation and finally generating the summary. To
tackle each subproblem, we created a gold standard dataset where human
annotators identified the citances in each of (up to) ten randomly sampled
citing papers for the RP.

%Muthu: moving annotation information down to data

% Min: do you need to define ``discourse facets''?
% done
Jaidka and Khoo \shortcite{jaidka2013}'s work on summarizing
information science articles indicated that most citations clearly
refer to one or more specific discourse facets of the cited paper. 
Discourse facets indicate the type of information described in the 
reference span. E.g., ``Aim'' indicates that the citation is about the 
Aim of the reference paper. In the CL domain, during our corpus 
construction, we identified that the discourse facets being cited 
were usually the aim of the paper, methods followed, and the results 
or implications of the work. Accordingly, we used a different set of 
discourse facets than BiomedSumm which suit our target domain of CL 
papers better. The resultant corpus should be viewed as a development 
corpus only, such that the community can enlarge it to a proper shared 
task with training, development and testing set divisions in the near 
future.

% Min: not relevant here.  
%% we plan to release a test set of documents for next year's evaluation,
%% we report $k$ fold cross-validated performance over the 10 documents
%% for the systems registered for participation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Corpus Construction}
\label{corpus}
A large and important portion of scholarly communication in the domain
of computational linguistics is publicly accessible and archived at
the ACL Anthology\footnote{\url{http://aclweb.org/anthology/}}.  The
texts from this archive are also under a Creative Commons license,
which allows unfettered access to the published works for any
purposes, including downstream research on summarization of its
contents.  

We thus view the ACL Anthology as a corpus and randomly sampled
published research papers as a base for building our annotated corpus.
In selecting materials for resultant corpus from the Anthology, we
wanted to enable citation-based summarization. To this end, with
consultation from the BiomedSumm organizers, we needed to ensure that
the reference paper was cited with appropriate diversity.

As of the corpus construction date (18 September 2014), the live
Anthology contained approximately 25K publications, exclusive of the
third-party papers hosted (i.e., with metadata but without the actual
. PDF of the paper) and extraneous files (i.e., front matter and full
volumes).  To ensure sufficient opportunity to use citation based
summarization, we further removed papers published after and including
2006, leaving 13.8K publications.  We randomized this list to remove
any ordering effects.  Starting from the top of the list, we used a
combination of Google Web and Scholar searches to approximate the
number of citations (i.e., citing papers (CP)). We retained any paper
with over 10 citations.  We vetted the citations to ensure that the
citation spread was at least a window of three years, as previous work
had indicated that citations over different time periods (with respect
to the publication date of the RP) exhibit different tendencies
% Min: BUG -- find the citation
\cite{N13-1067}.

We then used the title search facility of the ACL Anthology
Network\footnote{\url{http://clair.eecs.umich.edu/aan/index.php}}
(AAN, February 2013 version), to locate the paper. We inspected and
listed all citing papers' Anthology ID, title and year of publication.
We note the citation count from Google / Google Scholar and AAN differ
substantially.

To report the final list of citing papers, we strived to provide at
least three CP for each RP. We defined the following
criteria (in order or priority):
\begin{enumerate}
\item Non-list citation (i.e., at least one citation in the body of
  the CP for the RP not of the form [RP,a,b,c]);
\vspace{-.3cm}
\item The oldest and newest
citations within AAN; and, 
\vspace{-.3cm}
\item Citations from different years. 
\end{enumerate}

We included the oldest and newest citation regardless of criteria 1)
and 3) and included a randomized sample of up to 8 additional citing
paper IDs that met either criteria 1) and 3). 

The resulting final list was divided among the annotator group, whom
are a subset of the authors of this paper from NUS and NTU.  We used
the same scheme used by annotators of the BiomedSumm track's corpus.
Given each RP and up to 10 associated CPs, the annotation group was
instructed to find citations to the RP in each CP. Annotators followed
instructions used for BiomedSumm task annotation, to re-use the
resources created for BiomedSumm and reduce necessary effort.
Specifically, the citation text, citation marker, reference text, and
discourse facet were marked for each citation of the RP found in the
CP.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The CL-Summ Task}
This shared task proposes to solve same problems posed of the
BioMedSumm track, but in the domain of Computational Linguistics. This
task calls for summarization frameworks to build a structured summary
of a research paper -- which incorporates facet information (such as
Aims, Methods, Results and Implications) from the text of the paper,
and ``community summaries'' from its citing papers. \\

\noindent We define the {\it CL-Summ Task} as follows:

{\bf Given}: a topic, comprising of the PDF and extracted text of an RP and
up to 10 CPs.  In each provided CP, the citations to the RP (or
citances) have been identified. The information referenced in the RP
is also annotated.  Note that both the text, and the citations may be
noisy, and that there could be additional citing papers that were not
provided (due to sampling).

Output systems to perform the following tasks, where the numbering of
the task corresponds to those used in the BiomedSumm task.

\begin{itemize}
\item Task 1A: Identify the text span in the RP which corresponds to
  the citances from the CP. These may be of the granularity of a full
  sentence or several sentences (upto 5 sentences), and may be contiguous or not. It may
  also be a sentence fragment.
% Min: 5 what?  words?
 % done

\item Task 1B: Identify the discourse facet for every cited text span
  from a predefined set of facets.

% Min: BUG BUG Missing facets and their definitions!!
%done
Discourse facet is about the type of information described in the 
reference span. A maximum of 3 reference spans can be marked for 
every citance. In case these spans describe different different 
discourse facets, the most prevalent discourse facet is annotated.

\end{itemize}
 
% Min: Task 1 or Task 1A ?  Not clear
% Muthu: 1A. 1B deals with discourse facet identification
%KJ: added Task 2 description
{\bf Evaluation}: Assess Task 1A performance by using the
ROUGE~\cite{Lin:2004} score to compare the overlap of text spans in
the system output versus the gold standard created by human
annotators.

an additional task in BioMedSumm, which was tentative, and not 
advertised with this shared task, was:
\newpage
\begin{itemize}
\item Task 2: Generate a faceted summary of upto 250 words, of the 
reference paper, using itself and the citing papers.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Participating teams}
Nine teams expressed an interest in participating in the shared task
which are listed below in alphabetical order.

\begin{enumerate}
\item{{\bf CCS2014}, from the IDA Center for Computing Sciences,
  USA. They proposed to employ a language model based on the sections
  of the document to find referring text and related sentences in the
  cited document.}
\vspace{-.3cm}
\item{{\bf clair\_umich$^{*\$}$} from University of Michigan, Ann Arbor, USA.}
\vspace{-.3cm}
\item{{\bf IHMC}, A team from IHMC, USA.}
\vspace{-.3cm}
\item{{\bf IITKGP\_sum}, from Indian Institute of Technology,
  Kharagpur, India. They planned to use citation network structure and
  citation context analysis to summarize the scientific articles.}
\vspace{-.3cm}
\item{{\bf MQ$^{*\$}$}, from Macquarie University, Australia. They
  plan to use the same system that was used for the BiomedSumm track,
  with the exception that they will not incorporate domain knowledge
  (UMLS). For Task~1A they proposed to use similarity metrics to extract the
  top n sentences from the documents. For Task~1B they planned to use a
  logistic regression classifier. Next, for the bonus Task~2 they will 
  incorporate the distances from Task~1A to rank the sentences.
  }
% Min: BUG is there a task 2?
% Muthu: yes, there is. an optional / bonus task to 
% create a summary formt the citances identified
% Modified and appended the commented out part to baove text

\vspace{-.3cm}
\item{{\bf PolyAF}, from The Hong Kong Polytechnic University.}
\vspace{-.3cm}
\item{{\bf TabiBoun14}, from the Bogaziçi University, Turkey. They
  planned to modify an existing system for CL papers, which uses
% Min: face -> facet.  Correct?
%   LIBSVM as a classification tool for face classification. They also
% Muthu: yes, that makes sense
  LIBSVM as a classification tool for facet classification, and plan to 
  use cosine similarity to compare text spans.}
\vspace{-.3cm}
\item{{\bf Taln.UPF$^{*}$}, from Universitat Pompeu Fabra, Spain. They
  have proposed to adapt available summarization tools to scientific
  texts.}
\vspace{-.3cm}
\item{{\bf TXSUMM}, from University of Houston, Texas. Their system
  consists of applying similarity kernels in an attempt to better
  discriminate between candidate text spans (with sentence
  granularity). Their system uses an extractive, ranking method.}

\end{enumerate}

Three teams submitted system descriptions.  A further two (of the
three) submitted their findings.  The system descriptions and
self-reported task results are reported in the next sections (denoted
with `*' and `\$', respectively in the above text). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The clair\_umich System --- Comparing Overlap of Word Synsets}
\label{s:umich}
\subsection{Data Preprocessing}

For each RP, citing sentences were extracted from all its CP. Each
citing sentence was then matched to a text segment in the original
paper creating the final annotated dataset. The original source text
for the papers in the CL-Summ corpus was not sentence-segmented, which
made it difficult to compute evaluation metrics.

Data preprocessing of the CL-Summ corpus was done in the following way -- First, 
sentences from the reference papers were segmented and then matched to each of 
these source sentences to the CL-Summ annotation files. This yielded a fixed 
set of source sentences from the original files, a subset of which were 
matched to each citing sentence. In this way, given a citing sentence, 
matching sentences from the source paper were compared to the gold standard 
sentences matched from the source paper and compute precision / recall. 

The average number of source sentences matched for each citing sentence was 
1.28 (with standard deviation 1.92). The maximum number of source sentences 
matched for a citing sentence was 7. Given that the total number of source 
sentences for papers ranged from between 100 to 600, this made it a very 
challenging classification problem. 

\subsection{Baseline System}

% Min: changed source paper to RP.  Please check
% Muthu: thanks, this change was in the unpushed part with us.
The team first created a baseline system based on TF.IDF cosine
similarity. For any citing sentence, the system computed the TF.IDF
cosine similarity with all the sentences in the RP, thus the IDF
values differed across each of the 10 RPs.

\subsection{Supervised System}
% Muthu: Does WorNet need to be cited? or atleast the princeton web service
% could be added if that is the same version beoing used for this work
% Min: no need.  WN doesn't have to be cited; it's too famous
% Resolved!
The supervised system used knowledge-based features derived from WordNet,
syntactic dependency based features, and distributional features in addition 
to the simple lexical features like cosine similarity. These features are 
described below.

\begin{enumerate}
\item{\bf Lexical Features:} Two lexical features were used -- TF.IDF and the 
LCS (Longest Common Subsequence) between the citing sentence ($C$) and source 
sentence $S$, which is computed as:

\vspace{-.3cm}
\begin{eqnarray*}
  \frac{|LCS|}{min(|C|,|S|)}
\end{eqnarray*}

\item{\bf Knowledge Based Features:} The system also used set of features 
based on Wordnet similarity. Six wordnet based word similarity measures were 
combined to obtain six knowledge based sentence similarity features using the 
method proposed in \cite{Banea2012}. The wordnet based word similarity 
measures used are path similarity, WUP 
similarity~\cite{Wu:1994:VSL:981732.981751}, 
LCH similarity~\cite{leacock1998combining}, 
Resnik similarity~\cite{Resnik:1995:UIC:1625855.1625914}, Jiang-Conrath 
similarity~\cite{Jiang97taxonomySimilarity}, and Lin 
similarity~\cite{Lin:1998:IDS:645527.657297}. 

Given each of these similarity measures, the similarities between two 
sentences was computed by first creating a set of senses for each of the words 
in each of the sentences. Given these two sets of senses, the similarity 
score between citing sentence $C$ and source sentence $S$ was calculated 
as follows:

\vspace{-.3cm}
\begin{eqnarray*}
  sim_{wn}(C,S) = \frac{(\omega + \sum_{i=1}^{|\phi|}\phi_i) * (2|C||S|)}{|C|+|S|}
\end{eqnarray*}

Here $\omega$ is the number of shared senses between $C$ and $S$. The list 
$\phi$ contains the similarities of non-shared words in the shorter text, 
$\phi_i$ is the highest similarity score of the $i$th word among all the 
words of the lower text \cite{S13-1017}. 

\item{\bf Syntactic Features:} An additional feature based on similarity of 
dependency structures was used, by applying the method described in 
\cite{S13-1017}. The Stanford parser was used to obtain dependency parse all 
the citing sentences and source sentences. Given a candidate sentence pair, 
two syntactic dependencies were considered equal if they have the same 
dependency type, govering lemma, and dependent lemma. If $R_c$ and $R_s$ are 
the set of all dependency relations in $C$ and $S$, the dependency overlap 
score was computed using the formula:

\vspace{-.3cm}
\begin{eqnarray*}
  sim_{dep}(C,S) = \frac{2*|R_c \cap R_s| * |R_c||R_s|}{|R_c|+|R_s|}
\end{eqnarray*}

\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The MQ System --- Finding the Best Fit to a Citance}
\label{s:mq}
Given the text of a citance, the MQ system ranks the sentences of the 
reference paper according to its similarity to the citance. Every sentence 
and its citance was modeled as a vector and compared using cosine similarity. 
The team experimented with different forms of representing the information in 
the vectors, and different forms of using the similarity scores to perform the 
final sentence ranking.

\begin{figure*}
$$
\hbox{MMR} = \arg\max_{D_i\in R\setminus S}\left[\lambda(\hbox{sim}(D_i,Q)) -
(1-\lambda) \max_{D_j\in S} \hbox{sim}(D_i,D_j)\right]
$$  
\begin{quote}
Where:
\begin{itemize}
\item $Q$ is the citance text.
\item $R$ is the set of sentences in the document.
\item $S$ is the set of sentences that haven been chosen in the
  summary so far.  
\end{itemize}
\end{quote}
  \caption{Maximal Marginal Relevance (MMR)}
  \label{fig:mmr}
\end{figure*}

\subsection{Baseline -- Using TF.IDF}
\label{sec:tfidf}
For the baseline system (similar to the clair\_umich team), the TF.IDF
of all lowercased words was used, without removing stop
words. Separate TF.IDF statistics were computed for each reference
paper, using the set of sentences in the paper and the citance text of
all citing papers.

\subsection{Adding texts of the same topic}
\label{sec:topics}
Since the amount of text used to compute the TF.IDF in
Section~\ref{sec:tfidf} was relatively little, the complete text of
all citing papers was added, under the presumption that citing papers
are presumably of the same topic as the reference paper. By adding
this text we hope to include complementary information that can be
useful for extending and computing the IDF component.

\subsection{Adding context}
\label{sec:context}
In order to  extend the information of each sentence in the reference paper 
and further add to the approach in Section~\ref{sec:topics}, the text from 
the reference papers was added within a context window of 20 sentences by 
including the neighouring sentences, centered in the target sentence.


\subsection{Re-ranking using MMR}
\label{sec:mmr}
The last experiment used Maximal Marginal Relevance (MMR) \cite{Carbonell:1998} 
to rank the sentences. All sentences were represented as TF.IDF vectors 
of extended information as described in Section~\ref{sec:context}. Then, the 
final score of a sentence was the combination of the similarity with the 
citance and similarity of the other sentences of the summary according to the 
formula shown in Figure~\ref{fig:mmr}. A value of $\lambda=0.97$ was chosen.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Taln.UPF System}

\subsection{Pre-processing / documents preparation:}
The UPF system carried out the following set of preprocessing steps on the 
papers of each topic:
\begin{itemize}
\item{Sentence segmentation:} To identify candidate sentences that will be 
validated or rejected in the following pre-processing steps;
\item{Tokenizer and POS tagger:} Using the open-source GATE software
\item{Sentence sanitizer:} To remove incorrectly annotated sentences, 
relying on a set of rules and heuristics;
\item{Document structural analyzer:} To classify each sentence as belonging 
to one of the following document structural categories: Abstract, 
Introduction, Result\_Discussion, Experimental\_Procedure, 
Supplemental\_Data, Material\_Method, Conclusion, Acknowledgement\_Funding, 
and Reference;
\item{Sentence TF.IDF vector computation:} To associate to each sentence a 
TF.IDF vector where the IDF values are computed over all the papers of the 
related topic (up to 10 CP and one RP).
\end{itemize}

\subsection{Task 1A: Algorithm for identifying reference paper text spans for each citance}

% Min: edited, please check
%KJ added unpushed changes
\begin{itemize}
\item{For each citance its global citatance context span was considered as the union 
of the citance context spans} marked by human annotators (in this case, there 
was only one available human annotation, so no union was required).
\item{From the citing paper, those sentences were selected} which overlapped totally 
or partially the global citatance context span; these sentences were referred 
to as the citance context sentences (CtxSent1,..., CtxSentN),
\item{Citances were characterized by the document structural category 
associated with most of its citance context sentences 
(CtxSent1,..., CtxSentN)}. In case of tie in the number of occurrences of 
document structural categories among all the citance context sentences, the most 
frequently chosen document structural category for the citing paper was preferred. 
In case of persisting ties, the document structural category that is most frequent 
in the whole set of citing and reference papers was preferred.
\item{Each reference paper sentence (RefSent) was assigned a score} equal 
to the sum of its TF*IDF vector cosine similarity with each citance context 
sentence (CtxSent1,..., CtxSentN).
\item{The RefSent scores were weighted by the relative relevance} of this kind 
of link between document structural categories,  in the whole training corpus. 
For instance, if there is a citance associated to the INTRO that references a 
RefSent belonging to the Abstract and in the whole training corpus this situation 
occurs in 6.5\% of citance-referenced sentence pairs, the RefSent score is 
multiplied by 0.065, obtaining the final RefSent score.
\item{The first 3 reference paper sentences} (RefSents) with the highest 
final RefSent score were chosen as the reference paper text spans.
\end{itemize}

\subsection{Task 1B: Algorithm for identifying the discourse facet of the 
											cited text spans}
A linear-kernel SVM classifier was trained to associate each citance with one
 of the five text facets considered in Task 1B. Each citance was characterized 
 by lexical and semantic features extracted from the sentences belonging to 
 the citance context together with the sentences of the reference paper 
 selected as outcome of Task 1A.   Some of the features exploited were:
\begin{itemize}
\item{Relative number of sentences belonging to each document
  structural category;}
\vspace{-.3cm}
\item{Relative number of sentences belonging to the citance context or
  reference paper;}
\vspace{-.3cm}
\item{Relative number of POS;}
\vspace{-.3cm}
\item{Presence of key lexical patterns.}
\end{itemize}

\section{Evaluation and Results}
Two teams have submitted their results so far, as self-assessed using
ROUGE \cite{Lin:2004}. ROUGE (in specific, the ROUGE-L variant) is a
popular evaluation method for summarization systems that compares the
text output of the system against a set of target summaries. Since
ROUGE uses the actual contents words, and not the offset information
of the setences chosen by the annotation team, we expect non-zero
results for cases when a system chooses a sentence that is somewhat
similar to (but not identical) to one chosen by annotators.

The MQ system was an unsupervised system while clair\_umich system was
supervised. clair\_umich reports cross validated performance over the
10 topics while MQ evaluated their system over all 10 topics in a
single run.   
% this is not the case anymore. made the preceding sentence more generic.
The ROUGE-L scores have been calculated using the system output
of a set of selected sentences as the system summary, and comparing their overlap
against the target summaries are the sentences given by the
annotators. 
%For the clair\_umich system, the ROUGE-L scores were
%computed for each citing sentence in each annotation file separately
%and then averaged for a topic.  While the results are not strictly
%comparable, they do allow for cautious conclusions to be drawn.

The following paragraphs describe the results for Tasks~1A,~1B, and the
bonus Task~2 which was attempted by the MQ system.

\subsection{Task 1A: For each citance, identify the spans of text 
(cited text spans) in the RP}

\begin{table*}
\centering
	\begin{tabular}{|r|r|r|r|r|r|}
	\hline
	\multicolumn{3}{|c|}{MQ} & \multicolumn{3}{|c|}{clair\_umich}\\
	\hline
	P & R & $F_1$ & P & R & $F_1$\\
	\hline
% 	0.335 & 0.212 & 0.223 & 0.0 & 0.0 & 0.738\\
% Min: no reported R and P for clair\_umich?
% added in by Muthu
% Min2: from Rahul's email On Nov 10, 2014 12:12 AM, ``Rahul Jha'' <rahuljha@umich.edu> wrote:
% Min2: revises F_1 downward from .738 to .487
% Precision: 0.444
% Recall: 0.574
% F-score: 0.487
% 	0.212 & 0.335 & 0.223 & n/a & n/a & 0.738\\
 	0.212 & 0.335 & 0.223 & .444 & .574 & 0.487\\
	\hline
	\end{tabular}
\caption{Task~1A performance for the participating systems expressed as ROUGE-L score
 averaged over all topics.}
\label{tab:task1a}
\end{table*}

Table~\ref{tab:task1av2} shows the ROUGE-L $F_1$ scores of each individual 
reference document from the CL-Summ dataset.

\begin{table*}
  \centering
  \begin{tabular}{|l|r|r||l|r|r|}
  	\hline
	Paper ID & MQ  & clair\_umich & Paper ID & MQ System & clair\_umich \\
	\hline
	C90-2039 & 0.235 & 0.635 &	J00-3003 & 0.196 & 0.559\\
	C94-2154 & 0.288 & 0.536 &	J98-2005 & 0.101 & 0.344\\
	E03-1020 & 0.239 & 0.478 &	N01-1011 & 0.221 & 0.498\\
	H05-1115 & 0.350 & 0.375 &	P98-1081 & 0.200 & 0.367\\
	H89-2014 & 0.332 & 0.546 &	X96-1048 & 0.248 & 0.535\\
	\hline
  \end{tabular}
\caption{Task~1A ROUGE-L F1 scores for individual topics.}
\label{tab:task1av2}
\end{table*}
%change this from the text of Task 2 in my previous commit BUG
\subsection{Task 2: Generate a structured summary of the RP and all of the community discussion of the paper represented in the citances}

The MQ team performed an additional test to see whether information
from the citances were useful for building an extractive summary, as
is the case with the BiomedSumm data \cite{Molla:ALTA2014}. They
implemented extractive summarization systems with and without
information from the citances.  The summarizers without information
from the citances scored each sentence as the sum of the TF.IDF values
of the sentence elements. They tried the TF.IDF approach described in
Section~ref{sec:tfidf}.

The summarizers with information from the citances scored each candidate 
sentence $i$ on the basis of rank($i$,$c$) obtained in Task 1A, which has 
values between 0 (first sentence) and $n$ (last sentence), and represents 
the rank of sentence $i$ in citance $c$:

$$
\hbox{score}(i) = \sum_{c\in\hbox{citances}}1-\frac{\hbox{rank}(i,c)}{n}
$$

The summaries were evaluated using ROUGE-L, where the model summaries are 
the abstracts of the corresponding papers. Since paper X96-1048 of 
the SciSumm data did not have an abstract, it was omitted from this 
experiment.

An example excerpt from a target summary (Abstract) for the reference 
paper J03-3003 is:

\noindent\fbox{\parbox{.47\textwidth}{\it \footnotesize We describe a
    statistical approach for modeling dialogue acts in conversational
    speech, i.e., speech- act-like units such as STATEMENT, QUESTION,
    BACKCHANNEL, AGREEMENT, DISAGREE- MENT, and APOLOGY. Our model
    detects and predicts dialogue acts based on lexical,
    collocational, and prosodic cues, as well as on the discourse
    coherence of the dialogue act sequence. The dialogue model is
    based on treating the discourse structure of a conversation as a
    hidden Markov model and the individual dialogue acts as
    observations emanating from the model states. Constraints on the
    likely sequence of dialogue acts are modeled via a dialogue act
    n-gram...  We achieved good dialogue act labeling accuracy (65\%
    based on errorful, automatically recognized words and prosody, and
    71\% based on word transcripts, compared to a chance baseline
    accuracy of 35\% and human accuracy of 84\%) and a small reduction
    in word recognition error.}}

The MQ System's output baseline summary for the same reference paper is 20 
sentences long; below is an excerpt:

\noindent\fbox{\parbox{.47\textwidth}{\it \footnotesize Dialogue Act 
Modeling for Automatic Tagging and Recognition 
of Conversational Speech. In all these cases, DA labels would enrich the 
available input for higher-level processing of the spoken words. The 
relation between utterances and speaker turns is not one-to-one: a single 
turn can contain multiple utterances, and utterances can span more than 
one turn (e.g., in the case of backchanneling by the other speaker in 
midutterance). The most common of these are the AGREEMENT/ACCEPTS. One 
frequent example in our corpus was the distinction between BACKCHANNELS 
and AGREEMENTS (see Table 2), which share terms such as ``right'' and 
``yeah''. Networks compare to decision trees for the type of data studied 
here. Neural networks are worth investigating since they offer potential 
advantages over decision trees.}
}

Table~\ref{tab:task2v2} shows the breakdown of ROUGE-L $F_1$ scores per
document.

\begin{table*}
  \centering
  \begin{tabular}{|l|r|r|r||l|r|r|r|}
  \hline
	Paper ID & TF.IDF & Task 1A  & Task 1A & 	Paper ID & TF.IDF & Task 1A  & Task 1A  \\
			&	& TF.IDF & MMR & 	&	& TF.IDF & MMR \\
	\hline
	C90-2039\_TRAIN & 0.347 & 0.315 & 0.293 &	J00-3003\_TRAIN & 0.221 & 0.382 & 0.367\\
	C94-2154\_TRAIN & 0.095 & 0.123 & 0.120 & 	J98-2005\_TRAIN & 0.221 & 0.216 & 0.233\\
	E03-1020\_TRAIN & 0.189 & 0.189 & 0.196 &	N01-1011\_TRAIN & 0.187 & 0.268 & 0.284\\
	H05-1115\_TRAIN & 0.134 & 0.306 & 0.321 & 	P98-1081\_TRAIN & 0.241 & 0.210 & 0.206\\
\cline{5-8}
	H89-2014\_TRAIN & 0.294 & 0.319 & 0.320 &	Average & 0.214 & 0.259 & 0.260 \\
	\hline
  \end{tabular}
  \caption{ROUGE-L $F_1$ results for summaries generated by the MQ system.}
  \label{tab:task2v2}
\end{table*}

\section{Discussion}

\subsection{Comparing the MQ System with the BioMedSumm task}
Table~\ref{tab:task1a} compares the results of the MQ system's experiments 
with the SciSumm data, against the results from the BiomedSumm data. In all 
results the systems were designed to return 3 sentences, as specified in the 
shared task. All short sentences (under 50 characters) were ignored, to avoid 
including headings or mistakes made by the sentence segmentation algorithm.

\begin{table*}
  \centering
  \begin{tabular}{|l|r|r|r|c|r|r|r|c|}
  	\hline
	& \multicolumn{4}{|c|}{CL-Summ} & \multicolumn{4}{|c|}{BiomedSumm}\\
	\hline
	Run & P & R & $F_1$ & CI & P & R & $F_1$ & CI\\
	\hline
    TF.IDF & 0.198 & 0.316 & 0.211 & 0.185--0.240 & 0.326 & 0.273 & 0.279 & 0.265--0.293\\
	topics & 0.201 & 0.324 & 0.217 & 0.191--0.245 & 0.357 & 0.288 & 0.300
	& 0.285--0.316\\
	context & 0.214 & 0.339 & 0.225 & 0.197--0.255 & 0.372 & 0.291 & 0.308
	& 0.293--0.323\\
	MMR & 0.212 & 0.335 & 0.223 & 0.195--0.251 &  0.375 & 0.290 & 0.308 & 0.293--0.323\\ 
	\hline
  \end{tabular}
  \caption{ROUGE-L results of the MQ system runs for Task 1A.}
  \label{tab:task1a}
\end{table*}

The results show an improvement in both domains, with the exception
that MMR does not improve over the run that uses TF.IDF over context
in CL-Summ, whereas there is an improvement in BiomedSumm. The
absolute values are better in the BiomedSumm data, and looking at the
confidence intervals it can be presumed that the difference between
the best and the worst run is statistically significant in the
BiomedSumm data. The results in the CL-Summ data are poorer in general
and there are no statistically significant differences. However, this
may be an artifact of the small size of the corpus.  Overall, the
improvement of results in CL-Summ mirrors that of the BiomedSumm data,
so it can be suggested that on adding more information to the models
that compute TF.IDF, the results improve. It is expected that
alternative approaches, which gather related information to be added
for computing the vector models will produce even better results. The
results with MMR appears to be contradictory across the two domains,
but the difference is small and may not be statistically significant.

\subsection{Tweaking the Parameters --- the clair\_umich Baseline}
For any citing sentence, the TF.IDF cosine similarity was computed with all 
the sentences in the source paper, and any sentences that had a cosine 
similarity higher than a given threshold were added to the matched sentences. 
Table~\ref{tab:clairumichbaseline} shows the precision / recall for different 
values of the cosine threshold.
\begin{table}[h]
  \centering
  \begin{tabular}{|l|r|r|r|r|}
  	\hline
	Similarity & Precision & Recall & $F_1$ \\
	Threshold & & & \\
	\hline
	0.01 & 0.027 & 0.641 & 0.051\\
	0.05 & 0.048 & 0.426 & 0.087\\
	0.1 & 0.060 & 0.235 & 0.095\\
	0.2 & 0.079 & 0.081 & 0.080\\
	0.3 & 0.062 & 0.032 & 0.042\\
	0.4 & 0.022 & 0.085 & 0.012\\
	0.5 & 0.007 &  0.002 & 0.003\\
	\hline
  \end{tabular}
  \caption{Precision/Recall for different values of the cosine threshold 
  			for the baseline clair\_umich system.}
  \label{tab:clairumichbaseline}
\end{table}

The $F_1$ scores seems to reach a maximum at the similarity threshold of 0.1. 
The recall at the threshold of 0.1 is 0.23, while the precision is only 
0.06. This suggests that initial progress can be made on this problem by first 
removing these spurious matches that have high lexical similarity.
%combined the error analysis systems
\subsection{Error Analysis for the Participating Systems}

Some drawbacks were observed in the approach and evaluation for the MQ system. 
The example below illustrates the MQ system's output for task1a, for the 
reference paper H89-2014:

\noindent\fbox{\parbox{.47\textwidth}{\it \footnotesize ``The statistical 
methods can be described in terms of Markov models.''                    
``An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the 
training problem in terms of a "hidden" Markov model: that is, only the words 
of the training text are available, their corresponding categories are 
not known.''
``In this regard, word equivalence classes were used (Kupiec, 1989).'' 

The target sentence was:
``The work described here also makes use of a hidden Markov model.'' 
}}

The first sentence of the sample output was very similar to the target 
sentence. It was not the best match, but it was a close match, and an 
evaluation metric such as ROUGE would reward it. On the other hand, the 
second sentence, even though it talked about HMMs, it was not strictly 
about the approach used by the paper and therefore it should not be 
rewarded with a good score. However, ROUGE would be too lenient here. 
This is one of the issues identified by the MQ system in following a 
purely lexical approach.

In the clair\_umich system, a number of errors made by the baseline 
system are due to source sentences that match the words but differ 
slightly in their information content. 

\begin{figure}
\noindent\fbox{\parbox{.47\textwidth}{Citing text: ``use the BNC to build 
a co-occurrence graph for nouns, based on a  co-occurrence frequency 
threshold'' \\

\emph{True positives:}
\begin{itemize}
\vspace{-.2cm}
\item{\small ``Following the method in (Widdows and Dorow, 2002), we 
	build a graph in 
	which each node represents a noun and two nodes have an edge between them 
	if they co-occur in lists more than a given number of times.''}
\end{itemize}
\emph{False positives:}
\begin{itemize}
\vspace{-.2cm}
\item{\small ``Based on the intuition that nouns which co-occur in a list are
  often semantically related, we extract contexts of the form Noun,
  Noun,... and/or Noun, e.g. ``genomic DNA from rat, mouse and
  dog''.''}
\vspace{-.2cm}
\item{\small ``To detect the different areas of meaning in our local graphs,
  we use a cluster algorithm for graphs (Markov clustering, MCL)
  developed by van Dongen (2000).''}
\vspace{-.2cm}
\item{\small ``The algorithm is based on a graph model representing words and
  relationships between them.''}
\end{itemize}
}}
\caption{Lexically similar false positive sentences.}
\label{f:2}
\end{figure}
An example is shown in Figure~\ref{f:2}.  Here, even though the false
positive sentences contain the same lexical items (nouns,
co-occurrence, graph), they differ slightly in the facts presented.
Detection of such subtle differences in meaning might be challenging
for an automated system.

Another set of difficult sentences is when the citing sentence says something 
that is implied by the sentence in the RP, as evident in Figure~\ref{f:3}.

\begin{figure}
\noindent\fbox{\parbox{.47\textwidth}{Citing text: ``The line of our argument 
below follows a proof provided in ... for the maximum likelihood estimator based 
on nite tree distributions''

\emph{False negatives:}
\begin{itemize}
\item{\small ``We will show that in both cases the estimated probability is tight.''}
\end{itemize}
}}
\caption{Implied example.}
\label{f:3}
\end{figure}

% Min: Source paper changed to RP.  Ok?
% Ok
Here, the citing text mentions a proof from the RP, but to match the 
sentence in the RP, the system needs to understand that the act of 
showing something in a scientific paper constitutes a proof.

\section{Shortcomings and Limitations}
There were several errors and shortcomings of the dataset which were
identified in the process of annotating and parsing the corpus for use
by the participating systems. 
\begin{itemize}
\item{The use of ``...'' where text spans are snippets}: The use of
  ``...''  follows the BioMedSumm standard practice of indicating
  discontiguous texts.  In Citation Text and Reference Text fields,
  the ``...'' means that there is a gap between two text spans
  (citation spans or reference spans). They may be on different pages,
% Min: not clear what this means.  ``Gap might be a text''
%corrected
  so the gap might be a page number or a footnote. There might be a formula or a figure
  there, or some text encoding which is not a part of the annotation.
  However, this notation caused mismatches for sentences which used
  text from different parts of the same sentence.
\item{Small size of the training corpus:} The corpus comprised only a
  set of 10 topics, each with upto 10 citing documents. In this small
  dataset, participants were asked to conduct a 10-fold cross
  validation. The small size of the data set meant that there were no
  statistically significant results, but significance could only be
  guessed at from the overall trend of the data.
\item{Errors in parsing the file:} Some of the older PDF files, when
  parsed to text or XML, had such as misspelled words, spaces within
  words, sentences in the wrong place and so on. Unfortunately these
  errors were OCR parsing errors, and not within our control. We 
  recommended that  participants configure their string
  matching to be lenient enough to alleviate such problems.
\item{Errors in citation/reference offset numbers:} In the original
  annotations, citation/reference offset numbers were character-based,
  and relative to an XML encoding which was not shared in the task,
  and did not match with the offset numbers on the text-only, cleaned
  version of the document. Although the text versions of the source
  documents were shared with the intention to help the participants,
  this often made their tasks more difficult if their system was
  geared towards numerical and not system matching. A solution was
  found for reference offsets by revising them to sentence ID numbers
  based on available XML files from the clair\_umich system's
  pre-processing stage; however, the citation offsets remain
  character-based.
\item{Text encoding: Often, the text was not in UTF-8 format as
  expected}. Some participating teams, like the UPF, solved this by
  running the universal charset tool provided by Google Code over all
  the text and annotations in order to determine the right file
  encoding to use. It was found that some of the files were also in
  WINDOWS-1252 and GB18030 formats.
\item{Errors in file construction:} An automatic, open-source software
  was used to map the citation annotations from a software, Protege,
  to a text file.  However, participants identified several errors in
  the output - especially in cases where there was one-to-many mapping
  between citations and references.  Besides this, several annotation
  texts had no annotation ID (Citance Number field).
\end{itemize}

\section{Conclusions}
This paper describes the computational linguistics pilot task for the
faceted summarization of scholarly papers. We describe the three
systems participated in the shared task, and describe the evaluation
of two submitted runs. The teams used versions of TF.IDF as baselines.
The MQ system followed an unsupervised algorithm while
clair\_umich followed a supervised algorithm. For identifying
referenced text spans in reference papers, the best performance was
obtained by clair\_umich's supervised algorithm using lexical,
syntactic and knowledge-based features to calculate the overlap
between sentences in the citation span and the reference
paper. Although no system submitted results for Task 1B, the task
involving identifying the discourse facets of reference text, TALN.UPF
submitted an algorithm which they aim to implement.  Finally, an added
experiment by the MQ system sought to compare baseline summaries of
reference papers, based on a TF*IDF calculation, against gold standard
summaries, comprising the reference paper's abstracts.
 
The clair\_umich system incorporated WordNet synsets for expanding and 
comparing cited text with reference papers, and the use of syntactic features 
further enriched the calculation of overlap. On the other hand, the MQ system 
relied exclusively on reading and comparing texts. Furthermore, their system 
was originally built for the BioMedSumm task -- however, they had to discard 
some domain-specific features for this task. It is possible that the lack of 
domain knowledge, coupled with OCR-related and PDF parsing errors, affected 
the performance of their system in the CL domain.

This task is an initiative for encouraging the development of tools and 
approaches for scientific summarization. It helped us identify 
existing tools and resources to leverage on for this purpose
and also the hindrances which needed to be overcome in order to have a 
systematic and well-coordinated evaluation. However, with results of 
only for two systems, it is not possible to conjecture at what may be the 
better methods for summarizing CL research papers. The 
resources from this task, and its corpus, are freely available 
for interested research groups to experiment with; the corpus is 
first-of-its-kind summarization corpus for compuational linguistics.

The results of the pilot are encouraging: there seems to be ample
interest from the community and it seems possible to answer more
detailed methodological questions with a more detailed analysis and a
larger datasets.  We encourage the community to support a future
proposal to enlarge the pilot to a full scale shared task.  We plan a
systematic annotation of a training, development as well as test sets,
and the availability of more than one gold standard annotation, and
open-sourced tools and resources to support the efforts of
participating teams. We invite the community to join us in this
endeavour with any resources and time they can spare.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{tac2014}

\section{Acknowledgements}
 This Shared Task is supported in part by the Singapore National
 Research Foundation under its International Research Centre @
 Singapore Funding Initiative and administered by the IDM Programme
 Office.  The authors also acknowledge and thank the BiomedSumm
 organizers -- especially Lucy Vanderwende, Kevin B. Cohen, Prabha
 Yadav, and Hoa Trang Dang -- for lending their expertise in
 organizing this pilot.\\ The {\bf MQ system} was made possible thanks
 to a winter internship granted to Christopher Jones by the Department
 of Computing, Macquarie University. \\ The {\bf clair\_umich system}
 wishes to acknowledge the helpful suggestions of Ben King, Mohamed
 Abouelenien and Reed Coke. \\ The {\bf TALN.UPF} system is supported
 by the project Dr. Inventor (FP7-ICT-2013.8.1 611383), programa
 Ram\'on y Cajal 2009 (RYC-2009-04291), and the project
 TIN2012-38584-C06-03 Ministerio de Econom\'{\i}a y Competitividad,
 Secretar\'{\i}a de Estado de Investigaci\'on, Desarrollo e
 Innovaci\'on, Spain.

\bibliography{tac2014}

\end{document}
