%
% File acl-hlt2011.tex
%
% Contact: gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn
\documentclass[11pt]{article}
\usepackage{tac2014}
\usepackage{times}		
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[hyphens]{url}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{10.5cm}    % Expanding the titlebox

\title{The Computational Linguistics Summarization Pilot Task}

% Names appear in alphabetical order of last names except Kokil Jaidka
% who we all agree to be the first author of this work
% Participating teams may prefer a different order among the authors 
% from their team. Please let us know in this case.

% Muthu3: lets use ``scientific document summarization'' to refer the 
% generic task, as opposed to the specifc task of summarizing CL papers 
% in which case they would be reffred as CL-Summ, throughout this paper

% Min: please reformat to take up less space.  See for example:
% http://www.comp.nus.edu.sg/~kanmy/papers/airs2014.pdf
% in wing.nus/GITRepositories/zhanghc/AIRS2014.git for LaTeX source formatting
% Min: use \thanks for add in the other authoring information
% \vspace{-2cm}
\author{\\Kokil Jaidka$^{1}\thanks{\hspace{.2cm}Authors appear in alphabetical order, with the exception of the coordinator of the task, who is given first authorship.} $ , Muthu Kumar Chandrasekaran$^{2}$, Beatriz Fisas Elizalde$^{3}$, Rahul Jha$^{4}$, Christopher Jones$^{5}$ \\ { Min-Yen Kan}$^{2,6}${, Ankur Khanna}$^{2}${ , Diego Moll\'{a}-Aliod}$^{5}${ , Dragomir R. Radev}$^{4,7}$, \\ { Francesco Ronzano}$^{3}$ and { Horacio Saggion}$^{3}$ \\ 
\\
$^1$ Wee Kim Wee School of Communication \& Information, Nanyang Technological University, Singapore \\
$^2$ Web, IR \/ NLP Group, School of Computing, National University of Singapore, Singapore \\
$^3$ Universitat Pompeu Fabra, Barcelona, Spain\\
$^4$ Department of Electrical Engineering and Computer Science, University of Michigan, USA\\
$^5$ Faculty of Science and Engineering, Department of Computing, Macquarie University, Australia\\
$^6$ Interactive and Digital Media Institute, National University of Singapore, Singapore\\
$^7$ School of Information, University of Michigan, USA}
\begin{document}
\maketitle
\begin{abstract}
The Computational Linguistics (CL) Summarization Pilot Task was created to encourage a community effort 
to address the research problem of summarizing research articles as ``faceted summaries'' in the domain 
of computational linguistics. In this pilot stage, a hand-annotated set of citing papers was provided for 
ten reference papers, to help in automating the citation span and discourse facet identification problems.  
This paper details the corpus construction efforts by the organizers, and the participating teams who participated in the task-based evaluation. The annotated development corpus used for this pilot task is 
publicly available at:
\begin{sloppypar}
\url{https://github.com/WING-NUS/scisumm-corpus}
\end{sloppypar}
\end{abstract}
%The experience gleaned from the pilot will assist in the proper organization of future shared task where difficulties with annotations and scale can be addressed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

The Computational Linguistics Summarization Pilot task provides
resources aimed to encourage research on
%of scientific paper summarization
scientific document summarization.  Specifically, the task considers
summarization utilitizing the set of citation sentences (i.e.,
``citances'') that cite a reference article as a (community created)
summary of a topic or
paper~\cite{nanba2011classification,qazvinian2010identifying}. Citances
for a reference paper are considered a synopses of its key points and
also its key contributions and importance within an academic
community. An advantage of citances is that they embed meta-commentary
and offer a contextual, interpretative layer to the cited text. A
drawback of this approach is that citances usually do not consider the
context of the target user
\cite{jones2007automatic,teufel2002summarizing}, verify the claim of
the citation, nor provide context from the reference paper (in terms of
the kind of information cited, or where it is in the referenced
paper).

Existing scientific article summarization systems have approached the
task by automatically generating related work sections for a target
paper via a hierarchical topic tree~\cite{hoang2010towards},
generating model citation sentences~\cite{mohammad2009} or
implementing a literature review
framework~\cite{jaidka2013deconstructing}.  However, limited
evaluation resources -- i.e., human-created summaries -- means that
the efficacy of these approaches cannot be verified by others, hurting
the replicability of works in this domain. The goals of the
Computational Linguistics (CL) shared task was to highlight the
challenges and relevance of the scientific summarization, and provide
evaluation resources for advancing the state-of-the-art.

The CL pilot task was constructed by sampling papers from the
Association of Computational Linguistics' (ACL)
anthology~\cite{bird2008}.  This task was run concurrently with the
Text Analysis Conference 2014 (TAC '14), with the approval and
guidance by the TAC organizers, although not formally affiliated with
it.  It shares the same basic structure and guidelines as the formal
TAC 2014 Biomedical Summarization (BiomedSumm) track. We released a
training corpus of ``topics'' from CL research papers, each comprising
a reference paper along with sample papers that cited the reference
paper. Participants were then asked to participate in a task-based
evaluation and include a system description and self-reported results
as part of this report.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Background}

An ideal summary of a CL research paper would distill its overall
contribution in the state of the art through a discussion of its
goals, methods and results.  Previous work \cite{mohammad2009,abu2011}
in scientific document summarization have used citances from citing
papers (hereafter, {\it CPs}) to create a multidocument summary of a
reference paper (hereafter, {\it RP}). Their approach followed a
three-part process: finding the relevant documents (CPs), then
selecting sentences which justify the citation in the RP, and finally,
generating the summary. In this task, we follow this method and have
created a training corpus comprising human annotations for each of
these sub-problems. Human annotators identified the citances in each
of (up to) ten randomly sampled CPs for the RP.

Previous work also indicated that most citations clearly refer to one
or more specific discourse facets of the CP~\cite{jaidka2013}.
Discourse facets indicate the type of information described in the
reference span: e.g., ``Aim'' indicates that the citation concerns the
aims of the reference paper. From our exploration of the computational
linguistics domain, we observed that the discourse facets being cited
were usually the aim of the paper, its methods and the results or
implications of the work. We also applied these observations in annotating
discourse facets in our training corpus.
%moving this into next section
%the Drawing from these observations, we used a different set of discourse 
%facets than BiomedSumm which suit our target domain of CL papers better. 
%The resultant corpus should be viewed as a development corpus only, such 
%that the community can enlarge it to %a proper shared task with training, 
%development and testing set divisions in the near future.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Corpus Construction}
\label{corpus}

A large and important body of scholarly communication in the domain of
computational linguistics is publicly accessible and archived at the
ACL Anthology\footnote{\url{http://aclweb.org/anthology/}}. The texts
from this archive are also under a Creative Commons license, which
allows unfettered access to the works for any purpose, including
downstream research on summarization.

As of the corpus construction date (18 September 2014), the live
Anthology contained approximately 25K publications, exclusive of the
third-party papers hosted (i.e., with metadata but without the actual
PDF version of the paper) and extraneous files (i.e., front matter and
full volumes). We randomly sampled research papers for use as RPs
using the following procedure:
\begin{itemize}
\item{We considered only papers published after and including 2006,
  leaving 13.8K publications. We randomized this list to remove any
  ordering effects;}
\vspace{-.3cm}
\item{Starting from the top of the list, we used a combination of
  Google Web and Google Scholar searches to approximate the number of
  CPs for the RP.  We retained any paper as an RP if it was reported
  to have over 10 citations;}
\vspace{-.3cm}
\item{We vetted the citations to ensure that the citation spread was at least a 
window of three years, as previous work indicates that citations over different 
time periods (with respect to the publication date of the RP) exhibit different 
tendencies \cite{N13-1067};}
\vspace{-.3cm}
\item{We then used the title search facility of the ACL Anthology
  Network\footnote{\url{http://clair.eecs.umich.edu/aan/index.php}}
  (AAN, February 2013 version), to locate the paper and, inspected all
  citing papers' Anthology ID, title and year of publication. The
  citation count from Google / Google Scholar and AAN often differed
  substantially.}
\end{itemize}
For every RP, we aimed to provide at least three CPs based on the following 
criteria (in order or priority):
\begin{enumerate}
\item Non-list citation (i.e., at least one citation in the body of
  the CP for the RP not of the form [RP,a,b,c]);
\item The oldest and newest citations within AAN, and;
\vspace{-.3cm}
\item Citations from different years. 
\end{enumerate}

We included the oldest and newest citation regardless of criteria 1)
and 3), and included a randomized sample of up to 8 additional citing
paper IDs that met either criterion 1) and 3). The final list was
divided among the annotator group, who are a subset of the authors of
this paper, from the National University of Singapore and Nanyang
Technological University, Singapore. Annotators re-used the resources
created for BiomedSumm, which reduced the efforts required; however, a
different set of discourse facets were used to best represent the
content of computational linguistics research papers.  The resultant
corpus should be viewed as a development corpus only, such that later
efforts by the community can enlarge it to a proper shared task with
training, development and testing set divisions.

\subsection{Corpus Preprocessing}
The original source text for the papers in the CL-Summ corpus was not
sentence-segmented, which made it difficult to compute evaluation
metrics. Two of the participating teams, clair\_umich and TALN.UPF (see below),
performed significant corpus pre-processing to create a sanitized,
annotated dataset for their systems.

In the clair\_umich system, for each RP, citing sentences were
extracted from all its CPs.  Each CP sentence was matched to the RP to
create the final annotated dataset. Given a citing sentence, matching
sentences from the RP were compared to the gold standard RP sentences
to compute precision / recall.  On an average, each CP sentence
matched 1.28 RP sentences. The maximum number of matches was 7.

The UPF system performed the following sanitization process to
overcome corpus encoding issues:

\begin{enumerate}
\vspace{-.3cm}
\item {Automatic PDF-to-text conversion}: Conversion of PDF versions
  of the paper into text, by means of
  Poppler\footnote{\url{http://poppler.freedesktop.org/}}, a robust
  PDF-to-text converter;
\vspace{-.3cm}
\item {Manual verification of output}: Manual validation of the
  PDF-to-text conversion errors in order to get a clean textual
  version of each paper;
\vspace{-.6cm}
\item {Sentence splitter and Sentence Sanitizer}: Use of a rule-based
  sentence splitter and sanitizer to identify candidate sentences, and
  to remove incorrectly annotated sentences;
\vspace{-.3cm}
\item {Mapping annotations to clean textual versions}: Inspection of
  the textual contents of each of the annotation files, and manual
  mapping of the annotations to the clean textual version of each
  paper.
\end{enumerate}

These resulted in two sanitized versions of the initial corpus that
was shared as a part of this task. Both versions are shared, along
with the original, in the official repository of the CL Corpus with
the consent of the participants.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% MinCP: BUG Add a citation to the BioMedSumm Overview paper -- it's the best acknowledgment we can give them.
% MinCP2: Still missing citation!
% Kokil: added footnote to website. There is no publication as such.
% Muthu4: I think we can cite their notebook paper althought a paper is not available yet. Adding it here.
\section{The CL-Summ Task}
This shared task proposes to solve the same problems posed in the Biomedical Summarization Track of the
2014 Text Analysis Conference 2014 \cite{biomedsumm}\footnote{In case paper is not accessible 
 please see details here: \url{http://www.nist.gov/tac/2014/BiomedSumm/}}, 
but in the domain of Computational Linguistics. It poses the research problem of building a structured 
summary of a research paper -- which incorporates facet information (such as Aims, Methods, 
Results and Implications) from the text of the paper, and ``community summaries'' from its 
citing papers. \\

\noindent We define the {\it CL-Summ Task} as follows:

\noindent {\bf Given}: A topic, comprising of the PDF and extracted
text of an reference paper (RP) and up to 10 citing papers (CPs).  In
each provided CP, the citations to the RP (or citances) have been
identified and manually annotated. The information referenced in the
RP is also annotated.

\noindent {\bf Output}: Systems are required to perform the following
tasks, where the numbering of the task corresponds to those used in
the BiomedSumm task.

\begin{itemize}
\item Task 1A: Identify the text span in the RP which corresponds to the citances from the CP. 
  These may be of the granularity of a full   sentence or several sentences (up to five sentences), 
  and may be contiguous or not. It may also be a sentence fragment.
\vspace{-.3cm}
\item Task 1B: Identify the discourse facet for every cited text span from a predefined set of 
  facets. Discourse facets categorize the type of information described in the reference span. 
  A maximum of three reference spans can be marked for every citance. In case these spans describe 
  different discourse facets, the most prevalent discourse facet is annotated.
\end{itemize}

{\bf Evaluation}: Evaluate Task 1A performance by using the ROUGE~\cite{Lin:2004} score to 
compare the overlap of text spans in the system output versus the gold standard created by 
human annotators.

An additional task in BioMedSumm, which was not advertised with this shared task, was:

{\bf Task 2}: Generate a faceted summary of the reference paper of up to 250 words, 
by leveraging information from the citing papers.

Nine teams expressed an interest in participating in the shared task,
and three eventually submitted runs, system descriptions and
self-assessed results.  These three systems --- {\bf clair\_umich}
from University of Michigan; Ann Arbor, USA, {\bf MQ}, from Macquarie
University, Australia; and {\bf TALN.UPF}, from Universitat Pompeu
Fabra, Spain --- are described in the following sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Muthu2: ​I think, we can ​ ​remove teams that did not submit a result since simply registering 
% for the task may not merit a mention in the proceedings of a conference. We can push the 
% information about how many registered.
% We can move participation numbers to introduction and get rid of this section which would
% save a page.
% MinCP2: great idea, executing.
%% \section{Participating teams}
%% Nine teams expressed an interest in participating in the shared task,
%% and three eventually submitted runs, system descriptions and
%% self-assessed results.  The three are:
%% \begin{enumerate}
%% \item{{\bf clair\_umich} from University of Michigan, Ann Arbor, USA. They proposed a supervised system based on lexical, syntactic and knowledge-based features to calculate similarity scores between sentences in the CPs and the RP.}
%% \vspace{-.3cm}
%% \item{{\bf MQ}, from Macquarie University, Australia. They applied their system developed for the BiomedSumm Task,
%%   with the exception that they did not incorporate domain knowledge (in the form of UMLS). For Task~1A they used similarity metrics to extract the top $n$ sentences from the documents. For Task~1B they used a logistic regression classifier. For the optional Task~2 they incorporated the distances from Task~1A to rank the sentences.}
%% \vspace{-.3cm}
%% \item{{\bf Taln.UPF}, from Universitat Pompeu Fabra, Spain. They adapted available summarization tools to scientific texts.}
%
%\item{{\bf CCS2014}, from the IDA Center for Computing Sciences, USA. They proposed to employ a language model based on the %sections of the document to find referring text and related sentences in the cited document.}
%\vspace{-.3cm}
%\item{{\bf IHMC}, A team from IHMC, USA.}
%\vspace{-.3cm}
%\item{{\bf IITKGP\_sum}, from Indian Institute of Technology, Kharagpur, India. They planned to use citation network structure and
%  citation context analysis to summarize the scientific articles.}
%\vspace{-.3cm}
%\item{{\bf PolyAF}, from The Hong Kong Polytechnic University.}
%\vspace{-.3cm}
%\item{{\bf TabiBoun14}, from the Bogaziçi University, Turkey. They planned to modify an existing system for CL papers, which %uses LIBSVM as a classification tool for facet classification, and planned to use cosine similarity to compare text spans.}
%\vspace{-.3cm}
%\item{{\bf TXSUMM}, from University of Houston, Texas. Their system consists of applying similarity kernels in an attempt to %better discriminate between candidate text spans (with sentence granularity). Their system uses an extractive ranking method.}
%% \end{enumerate}
%% We report the system descriptions and task results for these systems in the following sections. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The clair\_umich System --- Comparing Overlap of Word Synsets}
\label{s:umich}

\subsection{Baseline System}

% Muthu2: Can we intergrate al three baselines which are small variations of tfxidf into
% a single section called baseline?

The team first created a baseline system based on the basic
information retrieval measure: {\it term frequency} $\times$ {\it
  inverse document frequency} (TF.IDF) cosine similarity. Note that
for any citing sentence, the system computed the TF.IDF cosine
similarity with all the sentences in the RP, thus the IDF values
differed across each of the 10 RPs.

\subsection{Supervised System}
The supervised system used knowledge-based features derived from WordNet, syntactic dependency based features, and distributional features in addition to the simple lexical features like cosine similarity. These features are described below.

% MinCP2: Is ``source'' synonymous with ``reference''?  If so, please
% note in the beginning. e.g.,
% Note that in this system description, ``source'' is synonymous with ``reference''.

% MinCP2: really try to fit all of the system descriptions into the
% same terminology.  This is your job as an editor.
%Kokil: sorry..fixed
\begin{enumerate}
\item{\bf Lexical Features:} Two lexical features were used -- TF.IDF and the LCS (Longest Common Subsequence) between the citing sentence ($C$) and reference sentence $S$, which is computed as:
\vspace{-.3cm}
\begin{eqnarray*}
  \frac{|LCS|}{min(|C|,|S|)}
\end{eqnarray*}
\item{\bf Knowledge Based Features:} Six WordNet-based similarity measures were combined to obtain six sentence similarity features~\cite{Banea2012}: path similarity, WUP similarity~\cite{Wu:1994:VSL:981732.981751}, LCH similarity~\cite{leacock1998combining}, Resnik similarity~\cite{Resnik:1995:UIC:1625855.1625914}, Jiang-Conrath similarity~\cite{Jiang97taxonomySimilarity}, and Lin similarity~\cite{Lin:1998:IDS:645527.657297}. 
Using these measures, they computed the similarities between the citing and reference sentences by creating a set of senses for each of the words in each sentence:
\vspace{-.3cm}
\begin{eqnarray*}
  sim_{wn}(C,R) = \frac{(\omega + \sum_{i=1}^{|\phi|}\phi_i) * (2|C||R|)}{|C|+|R|}
\end{eqnarray*}
Here $\omega$ is the number of shared senses between $C$ and $R$. The list $\phi$ contains the similarities of non-shared words in the shorter text, $\phi_i$ is the highest similarity score of the $i^{th}$ word among all the words of the lower text \cite{S13-1017}. 
\item{\bf Syntactic Features:} Given a candidate sentence pair, two syntactic dependencies were considered equal if they had the same dependency type, govering lemma, and dependent lemma \cite{S13-1017}. The Stanford parser was used to obtain dependency parses of all the citing sentences and reference sentences. Then, if $R_c$ and $R_r$ are the set of all dependency relations in $C$ and $R$, the dependency overlap score was computed using the formula:

\vspace{-.3cm}
\begin{eqnarray*}
  sim_{dep}(C,R) = \frac{2*|R_c \cap R_r| * |R_c||R_r|}{|R_c|+|R_r|}
\end{eqnarray*}
\end{enumerate}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The MQ System --- Finding the Best Fit to a Citance}
\label{s:mq}
Given the text of a citance, the MQ system ranked the sentences of the reference paper 
according to its similarity to the citance. Every sentence and its citance was modeled 
as a vector and compared using cosine similarity. 

\begin{figure*}
$$
\hbox{MMR} = \arg\max_{D_i\in R\setminus S}\left[\lambda(\hbox{sim}(D_i,Q)) -
(1-\lambda) \max_{D_j\in S} \hbox{sim}(D_i,D_j)\right]
$$  
\begin{quote}
Where:
\begin{itemize}
\item $Q$ is the citance text.
\item $R$ is the set of sentences in the document.
\item $S$ is the set of sentences that haven been chosen in the
  summary so far.  
\end{itemize}
\end{quote}
  \caption{Maximal Marginal Relevance (MMR) algorithm, as used in the {\it MQ} system.}
  \label{fig:mmr}
\end{figure*}

\textbf{Baseline -- Using TF.IDF}
For the baseline system, the TF.IDF of all lowercased words was used, 
without removing stop words (similar to the clair\_umich team). 
Separate TF.IDF statistics were computed for each reference paper, 
using the set of sentences in RP and the citance text of all 
citing papers (CPs).

\textbf{Adding texts of the same topic:}
Since the amount of text used to compute the TF.IDF was relatively little, 
it was presumed that citing papers are of the same topic. Accordingly the 
complete text of all citing papers was added in calculations for the IDF 
component.

\textbf{Adding context:}
To extend the information on each sentence in the RP,
%and further add to the approach in Section~\ref{sec:topics}, 
the text from the RPs was added within a context window of 
20 neighboring sentences to the target sentence from a CP. 

\textbf{MMR Re-ranking:}
The last experiment used Maximal Marginal Relevance (MMR)~\cite{Carbonell:1998} 
to rank the sentences. All sentences were represented as TF.IDF vectors of 
extended information as described in previous paragraph. The final score 
of a sentence was the combination of the similarity with the citance, and 
similarity with the other sentences of the summary, according to the formula 
shown in Figure~\ref{fig:mmr}. A value of $\lambda=0.97$ was chosen.

For all experiments, the systems were designed to return 3 sentences, as specified 
in the shared task. All short sentences (under 50 characters) were ignored, to 
avoid including headings or mistakes made by the sentence segmentation algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Muthu2: I think we should move details on the 2 versions of the corpus
%% created by UMICH and UPF to the end of 'Corpus Construction' and say 
%% "we have 3 versions of the corpus for download and use by the community
%% ...."

\section{The TALN.UPF System}

In the TALN.UPF system, the following text analysis tools were first used to pre-process the sanitized CL-Summ corpus:
\begin{enumerate}
\item \textbf{Tokenizer} and \textbf{POS tagger}: The GATE\footnote{https://gate.ac.uk/ie/annie.html} tool and its 
%% Muthu3: ANNIE expands as: a Nearly-New Information Extraction System
%% may need a footnote on what ANNIE is
ANNIE (A Nearly-New Information Extraction system) NLP tools for English were used to tokenize and tag the corpus.
\vspace{-.3cm}
\item \textbf{Sentence TF.IDF vector calculator}: A TF.IDF vector was generated for each sentence. The IDF values of the terms of each document were computed by considering the CPs and RP as a complete corpus.
\end{enumerate}

\subsection{Task 1A: Identifying RP text spans for each citance}
The TALN.UPF system implemented an algorithm to 
map every citation in the CP to one or more (up to three) 
\textit{reference text spans} from the RP. Sentences from the CP 
that overlapped partially or totally with the citation text were 
selected and referred to as the \textit{citation context} 
(CtxSent1,..., CtxSentN). For every sentence in the RP, the system 
associated a \textit{score} equal to the sum of the TF.IDF vector 
cosine similarities computed between that sentence and each sentence 
belonging to the citation context (CtxSent1,..., CtxSentN). Finally, 
the top $n$ sentences from the RP with the highest score were
chosen as the reference span. Conflicts in choice were resolved by 
preferring sentences that occurred in the same document section in 
the RP.
%% Muthu2: this sentence is not correct. It should be either: 
%% "by the prominence of document sections' selected reference spans;"
%% or 
%% "by the prominence of document sections selected as reference spans;"
Furthermore, scores for referencing sentences were weighted 
by the prominence of document sections selected reference spans; 
% for instance, if there is 6.5\% of all reference spans came from the 
% Abstract section, then the score for a sentences from the Abstract 
for instance, if there 6.5\% of all reference spans were sourced from the
Abstract section, then the score for a sentence from the Abstract 
of the RP is multiplied by 0.065. The system evaluated the performance 
of the algorithm with varying values of $n$, the number of sentences 
included in the reference span.

\subsection{Task 1B: Identifying the discourse facet  of the cited text spans}
Task 1B was cast as a sentence classification problem. From the
corpus, the system selected sentences from the CPs that overlapped
partially or totally with a manually annotated reference text span.
These CP sentences were then classified with the discourse facet of
the overlapping manually-annotated reference text span. This resulted
in a set of 266 CP sentences as distributed in
Table~\ref{table:task1bSentDistrib}.

\begin{table}[h]\footnotesize
  \begin{center}
  \begin{tabular}{ |l | c |}
    \hline
    Docset & Citing papers \\ \hline
    \textit{Aim} & 46 \\ \hline
    \textit{Hypothesis} & 1 \\ \hline
    \textit{Implication} & 25 \\ \hline
    \textit{Results} & 29 \\ \hline
    \textit{Method} & 165 \\ \hline
    \hline
    \textbf{TOTAL}: & 266 \\ \hline
  \end{tabular}
  \caption{Discourse facet of the sentences of cited papers belonging to a manually annotated reference text span.}
  \label{table:task1bSentDistrib}
  \end{center}
\end{table}

%% Muthu2: rewriting this para, please check if this is correct
Every sentence was modelled as a word vector of unigrams, 
bigrams, trigrams and lemmatized versions of all three.
%% Muthu2: what is preserving order of stopwords? Doesn't make sense to me.
%% they either preserved stopwords or preserved order of the words as they appeared.
% They preserved the order of stopwords as well. 
%% MinCP2: removed. also doesn't make sense to me.
% Order of stopwords was preserved as well.
Sentence classification performance was compared across 3 classifiers
-- \textit{Naive Bayes (NB)}, \textit{SVM} using a linear kernel and
\textit{Logistic Regression (LR)}. Results from a 10-fold cross
validation over the set of CP sentences listed in
Table~\ref{table:task1bSentDistrib} are shown in
Table~\ref{table:task1bAlgorithmComp}.  LR performed best with an
averaged $F_1$ of 0.719.
%Then, three sentence classifiers - \textit{Naive Bayes}, 
%\textit{SVM} with linear kernel and \textit{Logistic Regression} - were built and compared in a 10-fold 
%cross validation over the set of CP sentences (see Table~\ref{table:task1bSentDistrib}). 
%The results of this comparison are shown in Table~\ref{table:task1bAlgorithmComp}. 
%The best sentence classifier obtained an averaged F1 of 0.719. 

\section{Evaluation and Results}

\textbf{Results for Task 1:} All three teams submitted their
self-assessed results, using ROUGE \cite{Lin:2004} for
Task~1A. ROUGE-L, which compares system output against a set of target
summaries using the longest common subsequence of words, was
used. Since ROUGE uses actual content words, and not offsets, we
expect non-zero results when systems choose a sentence that is
somewhat similar to (but not identical) to one chosen by annotators.

For Task~1A, the MQ and TALN.UPF systems were unsupervised, while
clair\_umich system was supervised. The former two systems were
evaluated over all 10 topics in a single run, while clair\_umich
reported cross validated performance over the 10 topics.
Table~\ref{tab:task1a} shows the overall self-reported results,
averaged over all topics, while Table~\ref{tab:task1av2} shows
micro-level results, giving the ROUGE-L $F_1$ scores of each
individual reference document from the CL-Summ dataset. It should be noted that
both Table~\ref{tab:task1a} and Table~\ref{tab:task1av2} describe
the results of different implementations, and possibly different interpretations,
of the same recall metrics. Therefore, the differences in system performance should be treated
as a contextual rather than an absolute gap. In future events, we aim to overcome this
shortcoming of the task-based evaluation.

For Task~1B, the TALN.UPF system also followed a supervised approach
with 10-fold cross validation.  Self-reported results are shown in
Table~\ref{table:task1bAlgorithmComp}.  The ROUGE-L scores have been
calculated using the system output of a set of selected sentences as
the system summary, and comparing their overlap against the target
summaries are the sentences given by the annotators.

\begin{table}[t]
\centering
	\begin{tabular}{|r|r|r|r|}
	\hline
	& P & R & $F_1$ \\
	\hline
% 	0.335 & 0.212 & 0.223 & 0.0 & 0.0 & 0.738\\
% Min: no reported R and P for clair\_umich?
% added in by Muthu
% Min2: from Rahul's email On Nov 10, 2014 12:12 AM, ``Rahul Jha'' <rahuljha@umich.edu> wrote:
% Min2: revises F_1 downward from .738 to .487
% Precision: 0.444
% Recall: 0.574
% F-score: 0.487
% 	0.212 & 0.335 & 0.223 & n/a & n/a & 0.738\\
MQ & 	0.212& 0.335& 0.223 \\
clair\_umich & 0.444& 0.574& 0.487 \\
TALN.UPF & 0.194& 0.344& 0.225\\
\hline
\end{tabular}
\caption{Task~1A performance for the participating systems expressed as ROUGE-L score
 averaged over all topics.}
\label{tab:task1a}
\end{table}

\begin{table}[t]
  \centering
  \begin{tabular}{|l|r|r|r|}
  	\hline
	Paper ID & MQ  & clair\_umich & TALN.UPF \\ 
	\hline
	C90-2039 & 0.235 & 0.635 & 0.180 \\
	C94-2154 & 0.288 & 0.536 & 0.200 \\
	E03-1020 & 0.239 & 0.478 & 0.198 \\ 
	H05-1115 & 0.350 & 0.375 & 0.233 \\
	H89-2014 & 0.332 & 0.546 & 0.275 \\
        J00-3003 & 0.196 & 0.559& 0.263\\
        J98-2005 & 0.101 & 0.344& 0.196\\
        N01-1011 & 0.221 & 0.498& 0.254\\
        P98-1081 & 0.200 & 0.367& 0.211\\
        X96-1048 & 0.248 & 0.535& 0.240\\
	\hline
  \end{tabular}
\caption{Task~1A ROUGE-L F$_1$ scores for individual topics.}
\label{tab:task1av2}
\end{table}
%change this from the text of Task 2 in my previous commit BUG

\begin{table}[t]\footnotesize
  \begin{center}
  \begin{tabular}{ | l | c | c | c |}
    \hline
    Discourse facet & NB & SVM & \textbf{LR} \\ \hline
    \textit{Aim} & 0.725 & 0.734 & \textbf{0.732} \\ \hline
    \textit{Method} & 0.706 & 0.826 & \textbf{0.828} \\ \hline
    \textit{Implication} & 0.049 & 0.000 & \textbf{0.200} \\ \hline
    \textit{Results} & 0.509 & 0,533 & \textbf{0.533} \\ \hline
    \textit{Hypothesis} & 0.024 & 0.000 &\textbf{ 0.000} \\ \hline
    \hline
    \textbf{WEIGHED AVG. $F_1$} & 0.623 & 0.698 & \textbf{0.719} \\ \hline
  \end{tabular}
  \caption{Task~1B self-evaluation for TALN.UPF: F$_1$ classification performance comparison.}
  \label{table:task1bAlgorithmComp}
  \end{center}
\end{table}

\textbf{Results for Task 2:} The MQ team performed an additional test
to see whether information from the citances were useful for building
an extractive summary, as this was also the case with the BiomedSumm
task \cite{Molla:ALTA2014}.  They implemented extractive summarization
systems with and without citance information.  The summarizers without
information from the citances scored each sentence as the sum of the
TF.IDF values of the sentence elements. They tried the TF.IDF approach
described in Section~\ref{s:mq}.

The summarizers with information from the citances scored each candidate sentence 
$i$ on the basis of $rank(i,c)$ obtained in Task~1A, which yields values between 0 
(first sentence) and $n$ (last sentence), and represents the rank of sentence $i$ 
in citance $c$:
\vspace{-3mm}
$$
score(i) = \sum_{c\in citances}1-\frac{rank(i,c)}{n}
$$

The summaries were again evaluated using ROUGE-L, where the model
summaries are the abstracts of the corresponding papers. Since paper
X96-1048 of the SciSumm data did not have an abstract, it was omitted
from this experiment.

An example excerpt from a target summary (Abstract) for the reference 
paper J03-3003 is:

\noindent\fbox{\parbox{.47\textwidth}{\it \footnotesize We describe a statistical 
approach for modeling dialogue acts in conversational speech, i.e., speech- act-like 
units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY. 
Our model detects and predicts dialogue acts based on lexical, collocational, and 
prosodic cues, as well as on the discourse coherence of the dialogue act sequence. 
The dialogue model is based on treating the discourse structure of a conversation as 
a hidden Markov model and the individual dialogue acts as observations emanating from 
the model states. Constraints on the likely sequence of dialogue acts are modeled via 
a dialogue act n-gram...  We achieved good dialogue act labeling accuracy (65\% based 
on errorful, automatically recognized words and prosody, and 71\% based on word transcripts, 
compared to a chance baseline accuracy of 35\% and human accuracy of 84\%) and a small 
reduction in word recognition error.}}

The MQ System's output baseline summary for the same reference paper is 20 sentences long; 
below is an excerpt:

\noindent\fbox{\parbox{.47\textwidth}{\it \footnotesize Dialogue Act 
Modeling for Automatic Tagging and Recognition 
of Conversational Speech. In all these cases, DA labels would enrich the available input 
for higher-level processing of the spoken words. The relation between utterances and speaker 
turns is not one-to-one: a single turn can contain multiple utterances, and utterances can 
span more than one turn (e.g., in the case of backchanneling by the other speaker in midutterance). 
The most common of these are the AGREEMENT/ACCEPTS. One frequent example in our corpus was the 
distinction between BACKCHANNELS and AGREEMENTS (see Table 2), which share terms such as ``right'' 
and ``yeah''. Networks compare to decision trees for the type of data studied here. Neural networks 
are worth investigating since they offer potential advantages over decision trees.}
}

Table~\ref{tab:task2v2} shows the breakdown of ROUGE-L $F_1$ scores per
document.

\begin{table*}
  \centering
  \begin{tabular}{|l|r|r|r||l|r|r|r|}
  \hline
	Paper ID & TF.IDF & Task 1A  & Task 1A & 	Paper ID & TF.IDF & Task 1A  & Task 1A  \\
			&	& TF.IDF & MMR & 	&	& TF.IDF & MMR \\
	\hline
	C90-2039\_TRAIN & 0.347 & 0.315 & 0.293 &	J00-3003\_TRAIN & 0.221 & 0.382 & 0.367\\
	C94-2154\_TRAIN & 0.095 & 0.123 & 0.120 & 	J98-2005\_TRAIN & 0.221 & 0.216 & 0.233\\
	E03-1020\_TRAIN & 0.189 & 0.189 & 0.196 &	N01-1011\_TRAIN & 0.187 & 0.268 & 0.284\\
	H05-1115\_TRAIN & 0.134 & 0.306 & 0.321 & 	P98-1081\_TRAIN & 0.241 & 0.210 & 0.206\\
\cline{5-8}
	H89-2014\_TRAIN & 0.294 & 0.319 & 0.320 &	Average & 0.214 & 0.259 & 0.260 \\
	\hline
  \end{tabular}
  \caption{ROUGE-L $F_1$ results for summaries generated by the MQ system.}
  \label{tab:task2v2}
\end{table*}

\section{Discussion}

We look at each of the three systems in detail in the following
discussion.

\subsection{MQ System Performance: BioMedSumm Vs. CL-Summ}

Since the tasks of CL-Summ parallel those of Biomedsumm it is
interesting to compare results across the two domains. MQ, which
performed both Tasks~1 and 2, is used for this comparison.
Table~\ref{tab:task1amq} compares MQ's results over the BiomedSumm
corpus against those form the CL-Summ corpus, over the runs which
featured iterative improvements.

\begin{table*}
  \centering
  \begin{tabular}{|l|r|r|r|c|r|r|r|c|}
  	\hline
	& \multicolumn{4}{|c|}{CL-Summ} & \multicolumn{4}{|c|}{BiomedSumm}\\
	\hline
	Run & P & R & $F_1$ & CI & P & R & $F_1$ & CI\\
	\hline
    TF.IDF & 0.198 & 0.316 & 0.211 & 0.185--0.240 & 0.326 & 0.273 & 0.279 & 0.265--0.293\\
	topics & 0.201 & 0.324 & 0.217 & 0.191--0.245 & 0.357 & 0.288 & 0.300
	& 0.285--0.316\\
	context & 0.214 & 0.339 & 0.225 & 0.197--0.255 & 0.372 & 0.291 & 0.308
	& 0.293--0.323\\
	MMR & 0.212 & 0.335 & 0.223 & 0.195--0.251 &  0.375 & 0.290 & 0.308 & 0.293--0.323\\ 
	\hline
  \end{tabular}
  \caption{ROUGE-L results of the MQ system runs for Task 1A.}
  \label{tab:task1amq}
\end{table*}

The results show an improvement in both domains, with the exception
that MMR does not improve over the run that uses TF.IDF over context
in CL-Summ, whereas there is an improvement in BiomedSumm. The
absolute values are better in the BiomedSumm data, and looking at the
confidence intervals it can be presumed that the difference between
the best and the worst run is statistically significant in the
BiomedSumm data.  The results over the CL-Summ data are worse in
general but there are no statistically significant differences.
Overall, the improvement of results in CL-Summ mirrors that of the
BiomedSumm data, so it can be suggested that on adding more
information to the models that compute TF.IDF, the results improve. It
is expected that alternative approaches, which gather related
information to be added for computing the vector models will produce
even better results. The results with MMR is mixed across the two
domains, but the difference is small and may not be statistically
significant.

\subsection{Tweaking Parameters --- clair\_umich Baseline}
In the clair\_umich baseline, for any citing sentence, the TF.IDF
cosine similarity was computed with all the sentences in the source
paper, and any sentences that had a cosine similarity higher than a
given threshold were added to the matched
sentences. Table~\ref{tab:clairumichbaseline} shows the precision /
recall values for different cosine thresholds.
\begin{table}[h]
  \centering
  \begin{tabular}{|l|r|r|r|r|}
  	\hline
	Similarity & Precision & Recall & $F_1$ \\
	Threshold & & & \\
	\hline
	0.01 & 0.027 & 0.641 & 0.051\\
	0.05 & 0.048 & 0.426 & 0.087\\
	0.1 & 0.060 & 0.235 & 0.095\\
	0.2 & 0.079 & 0.081 & 0.080\\
	0.3 & 0.062 & 0.032 & 0.042\\
	0.4 & 0.022 & 0.085 & 0.012\\
	0.5 & 0.007 &  0.002 & 0.003\\
	\hline
  \end{tabular}
  \caption{Precision / Recall for different values of the cosine threshold 
  			for the baseline clair\_umich system.}
  \label{tab:clairumichbaseline}
\end{table}

The $F_1$ scores reach a maximum at a threshold of 0.1. The recall at
the threshold of 0.1 is 0.23, while the precision is only 0.06. This
suggests that more efforts may tackle this problem by first removing
these spurious matches that have high lexical similarity.
%combined the error analysis systems

\subsection{Tweaking Parameters --- TALN.UPF}
TALN.UPF's algorithm for Task~1A chooses the top $n$ sentences of the
cited paper with the highest \textit{score} as reference text
spans. They also experimented with various settings for $n$ to
evaluate and tune the performance of their approach.
Table~\ref{table:task1aEval} shows that on average the best result
(TOP 4) is obtained when the top 4 sentences that are most similar to the
citation context are selected to make up the reference span.
%From Table~\ref{table:task1aEval} we can notice that our approach obtains the best 
%average result when we select a reference text span that includes the 4 sentences 
%(TOP 4) that are most similar to the citation context.

\begin{table}[h]\footnotesize
  \begin{center}
  \begin{tabular}{ | l | c | c | c | c | }
    \hline
    Paper~ID & Top 2 & Top 3 & Top 4 & Top 5 \\ \hline
    C90-2039 & 0.087 & 0.097 & 0.153 & 0.134 \\ \hline
    C94-2154 & 0.000 & 0.096 & 0.110 & 0101 \\ \hline
    E03-1020 & 0.087 & 0.099 & 0.106 & 0.093 \\ \hline
    H05-1115 & 0.017 & 0.112 & 0.106 & 0.093 \\ \hline
    H89-2014 & 0.214 & 0.196 & 0.178 & 0.152 \\ \hline
    J00-3003 & 0.121 & 0.103 & 0.084 & 0.072 \\ \hline
    J98-2005 & 0.145 & 0.105 & 0.083 & 0.068 \\ \hline
    N01-1011 & 0.125 & 0.107 & 0.128 & 0.167 \\ \hline
    P98-1081 & 0.104 & 0.105 & 0.086 & 0.072 \\ \hline
    X96-1048 & 0.205 & 0.175 & 0.153 & 0.156 \\ \hline
    \textbf{Average:}  & 0.111 & 0.120 & 0.121 & 0.116 \\ \hline
  \end{tabular}
  \caption{Variation of the $F_1$ score when the reference text span is identified by 
  considering the 2/3/4/5 sentences of the cited paper with highest similarity to
  the citation context.}
  \label{table:task1aEval}
  \end{center}
\end{table}


\subsection{Error Analysis for the Participating Systems}

Several drawbacks were observed in the approach and evaluation for the
MQ system. We illustrate this in the example in Figure~\ref{f:error1} for 
Task~1A (for H89-2014).


\begin{figure}[h]
\noindent\fbox{\parbox{.47\textwidth}{\it\footnotesize (1) ``The statistical 
methods can be described in terms of Markov models.''                    

(2) ``An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the 
training problem in terms of a ``hidden'' Markov model: that is, only the words 
of the training text are available, their corresponding categories are 
not known.''

(3) ``In this regard, word equivalence classes were used (Kupiec, 1989).'' 

(TS) The target sentence was: ``The work described here also makes use
of a hidden Markov model.''  }}
\label{f:error1}
\caption{Overzealous vocabulary matching problems with ROUGE as observed by MQ.}
\end{figure}

The first sentence of the sample output was very similar to the target
sentence. It was not the best match, but it was a close match, and an
evaluation metric such as ROUGE would reward it. On the other hand,
the second sentence -- even though it discussed HMMs -- was not
strictly about the approach used by the paper and therefore it should
not be rewarded with a good score. However, ROUGE is too lenient for
this example, highlighting issues identified by the MQ system, as they
followed a purely lexical approach.

\begin{figure}[h]
\noindent\fbox{\parbox{.47\textwidth}{Citing text: ``use the BNC to build a co-occurrence graph for nouns, based on a  co-occurrence frequency threshold'' \\

\emph{True positives:}
\begin{itemize}
\vspace{-.2cm}
\item{\small ``Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times.''}
\end{itemize}
\emph{False positives:}
\begin{itemize}
\vspace{-.2cm}
\item{\small ``Based on the intuition that nouns which co-occur in a list are
  often semantically related, we extract contexts of the form Noun,
  Noun,... and/or Noun, e.g. ``genomic DNA from rat, mouse and
  dog''.''}
\vspace{-.2cm}
\item{\small ``To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).''}
\vspace{-.2cm}
\item{\small ``The algorithm is based on a graph model representing words and relationships between them.''}
\end{itemize}
}}
\caption{Lexically similar false positive sentences.}
\vspace{-1cm}
\label{f:2}
\end{figure}

In the clair\_umich system, a number of errors made by the baseline
system are due to the selection source sentences that match the words
but differ slightly in their information content.  An example is shown
in Figure~\ref{f:2}.  Here, even though the false positive sentences
contain the same lexical items (``noun'', ``co-occur'', ``graph''),
they differ slightly in the facts presented. The detection of such subtle
differences in meaning is challenging for an automated system.

Another set of difficulties arise when the citing sentence says
something that is implied by the sentence in the RP, as evident in
Figure~\ref{f:3}.  Here, the citing text mentions a proof from the RP,
but to match the sentence in the RP, the system needs to understand
that the act of showing something in a scientific paper constitutes a
proof.

\begin{figure}[h]
\noindent\fbox{\parbox{.47\textwidth}{ 
    Citing text: ``The line of our
    argument below follows a proof provided in ... for the maximum
    likelihood estimator based on nite tree distributions''

\emph{False negatives:}
\begin{itemize}
\item{\small ``We will show that in both cases the estimated probability is tight.''}
\end{itemize}
}}
\caption{Implied example.}
\label{f:3}
\end{figure}

TALN.UPF's top $n$ algorithm for finding the reference spans makes
errors in selecting 3$^{rd}$ and 4$^{th}$ sentences. In
particular, in 4 document sets (H05-1115, H89-2014, J00-3003 and
X96-1048) they notice that the best F$_1$ score is obtained by
selecting only the top 2 sentences since the 3$^{rd}$, 4$^{th}$ and
5$^{th}$ most similar sentences do not overlap with the gold
standard. The classification algorithm used in Task~1B also encounters
sparse occurrences of the correct \textit{implication} class, causing
it to be particularly difficult.

\section{Shortcomings and Limitations}
\label{sec:limitations}
The participating teams helped us to address the inherent errors in the CL corpus, which were identified in the process of annotating and parsing the corpus for use in the task-based evaluations:
\begin{itemize}
\item \textbf{Text encoding}: Often, the text was not in UTF-8 format as expected. The TALN.UPF team solved this by running the universal charset tool provided by Google Code over all the text and annotations in order to determine the right file encoding to use. It was found that some of the files were also in \textit{WINDOWS-1252} and \textit{GB18030} formats, thus making difficult the implementation of an automated homogeneous textual processing pipeline.
\vspace{-.3cm}
\item \textbf{Content}: Some of the older PDF files, when parsed to text or XML, presented several text formatting issues: hyphenation problems, words not separated by blank spaces, page headers and footnotes included in the textual flow, misspelled words, spaces within words, sentences in the wrong place and so on. Unfortunately these errors were OCR parsing errors, and not within our control. We recommended that  participants configure their string matching to be lenient enough to alleviate such problems.
\vspace{-.3cm}
\item \textbf{Errors in citation / reference offsets:} In the original annotations, citation / reference offset numbers were character-based, and relative to an XML encoding which was not shared in the task, and did not match with the offset numbers on the text-only, cleaned version of the document. Although the text versions of the source documents were shared with the intention to help the participants, this often made their tasks more difficult if their system was geared towards numerical and not system matching. A solution was found for reference offsets by revising them to sentence ID numbers based on available XML files from the clair\_umich team's donated pre-processing stage; however, the citation offsets remain character-based. As a consequence, in order to retrieve the annotated texts, other systems, such as TALN-UPF, manually searched through citing documents to identify the correct offset. The clair\_umich system created an automatic program to generate sentence offsets.
\vspace{-.3cm}
\item \textbf{Discontiguous texts}: The use of ``...''  follows the BioMedSumm standard practice of indicating discontiguous texts, meaning that there was a gap between two text spans (citation spans or reference spans). The gap might be because text moves onto a new page. Sometimes there was a formula, page number or figure between two text spans which is not a part of the annotation. However, this notation caused mismatches for sentences which used text from different parts of the same sentence.
\vspace{-.3cm}
\item \textbf{Small corpus:} The corpus comprised only a set of 10 topics, each with up to 10 citing documents. In this small
  corpus, participants were asked to conduct a 10-fold cross validation. The small size of the data set meant that there were no
  statistically significant results.  Overall trends should be regarded as indicative only.
\vspace{-.3cm}
\item \textbf{Errors in file construction:} An automatic, open-source software was used to map the citation annotations from the adopted annotation software, Protege, to a text file.  However, participants identified several errors in the output -- especially in cases where there was one-to-many mapping between citations and references.  Besides this, several annotation texts had no annotation ID (Citance Number field).
\end{itemize}

% MinCP: edited past here.  Other parts of the paper not edited.
% MinCP: BUG the writing here is pretty sloppy.  Please fix
% appropriately.  If you are going to write paper which many people
% will potentially refer to you should write it well, especially the
% abstract, intro and conclusion.
%
% MinCP: Please include TALN UPF in the descriptions and let me have a
% look again later.
%Kokil: done
\section{Conclusion}
Three systems participated in the CL Pilot Task, consisting of
Tasks~1A,~1B and 2. All three teams used versions of TF.IDF as
baselines.  For the citation span identification task, MQ and TALN.UPF
implemented unsupervised algorithms, while the the clair\_umich system
decided on a supervised approach. Overall, in this first task,
clair\_umich's supervised algorithm performed best, using lexical,
syntactic and knowledge-based features to calculate the overlap
between sentences in the citation span and the reference paper.
% MinCP: Doesn't make sense to include.  Omit descriptions of
% ``planned'' things that didn't happen.
% Although no system submitted results for Task 1B, the task involving
% identifying the discourse facets of reference text, TALN.UPF
% submitted an algorithm which they aimed to implement.
The clair\_umich system incorporated WordNet synsets for expanding and
comparing cited text with reference papers, and used syntactic
features to further enrich overlap calculations. In contrast, the
TALN.UPF and MQ system were purely lexical-based. The MQ system was a
simple port of the system originally built for the BioMedSumm task --
but with some domain-specific features discarded for this task. We
believe that the lack of domain knowledge, coupled with OCR-related
and PDF parsing errors, affected its performance for the CL task.

TALN.UPF attempted the second part of the task (Task~1B) for
identifying the discourse facet being cited. They compared the
performance of three sentence classifiers and found that the best
performance was obtained using logistic regression on lemmatized word
features.

Task~2 was attemped by the MQ team.  They compared the baseline
summaries of reference papers against gold standard summaries, based
on TF.IDF calculations. In comparison with MQ's results on the
BioMedSumm task, the results were inconclusive to state whether or not
the system's features were actually aiding in generating better gold
standard summaries. This may be an artifact of the small size of the
corpus, but it does suggest that different domains of scientific
research have different styles and features in their scientific
summaries. Methods that worked in the biomedical domain do not seem to
have fared well in computational linguistics. \\

We deem our pilot task a success, as it has spurred the development of
tools and approaches for scientific summarization for our own domain
of computational linguistics.
% MinCP2: Dropped, not helpful
% It helped us identify existing tools and resources to leverage on
% for this purpose and also the hindrances which needed to be overcome
% in order to have a systematic and well-coordinated evaluation.
However, with the limited size of the corpus and lack of a proper test
corpus, we only have indicative results and do not conjecture about
the optimal methods for summarizing CL research papers.  Importantly,
the resources from this task we feel are important artifacts for the
community going forward.  In particular, the annotated computational
linguistics corpus -- and pre-processed versions generously shared by
the clair\_umich and TALN.UPF teams -- are freely available for
researchers to use.
% MinCP: need to update with current plans for 2016 TAC, rather than
% just 2015.  Some of this should make it into the discussion and not
% the conclusion.  The conclusion should just contain verifiable
% facts, and the call for help should appeare elsewhere.
%Kokil: tried.
The results of the pilot are encouraging: there seems to be ample
interest from the community and it seems possible to answer more
detailed methodological questions with more detailed analyses over
larger datasets.  In a future task in 2016, we plan a systematic
annotation of a training, development and test sets, and have planned
for more than one gold standard annotation. To address possible discrepancies in
different interpretations of the evaluation metrics, we plan to have a single 
implementation of the evaluation metrics for comparing system performance. 
We hope also to be able to provide open-sourced tools and resources
to support the efforts of participating teams.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{tac2014}

\section{Acknowledgments}
This shared task is supported in part by the Singapore National
Research Foundation under its International Research Centre @
Singapore Funding Initiative and administered by the IDM Programme
Office.  The authors also acknowledge and thank the BiomedSumm
organizers -- especially Lucy Vanderwende, Kevin B. Cohen, Prabha
Yadav, and Hoa Trang Dang -- for lending their expertise in organizing
this pilot.

The {\bf MQ system} was made possible thanks to a winter internship
granted to Christopher Jones by the Department of Computing, Macquarie
University.

The {\bf clair\_umich system} wishes to acknowledge the helpful
suggestions of Ben King, Mohamed Abouelenien and Reed Coke.

The {\bf TALN.UPF system} is supported by the EU project Dr. Inventor
(FP7-ICT-2013.8.1 project number 611383), the Project
TIN2012-38584-C06-03 of the Ministerio de Econom\'{\i}a y
Competitividad, Secretar\'{\i}a de Estado de Investigaci\'on,
Desarrollo e Innovaci\'on, Spain and the Program Ram\'on y Cajal 2009
(RYC-2009-04291).

\bibliography{tac2014}

\end{document}
